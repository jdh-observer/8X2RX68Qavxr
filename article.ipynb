{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "": []
     }
    },
    "tags": [
     "title"
    ]
   },
   "source": [
    "# In search of an interpretative environment for digital traces: the building of Arvest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "copyright"
    ]
   },
   "source": [
    "[![cc-by](https://licensebuttons.net/l/by/4.0/88x31.png)](https://creativecommons.org/licenses/by/4.0/) \n",
    "©<AUTHOR or ORGANIZATION / FUNDER>. Published by De Gruyter in cooperation with the University of Luxembourg Centre for Contemporary and Digital History. This is an Open Access article distributed under the terms of the [Creative Commons Attribution License CC-BY](https://creativecommons.org/licenses/by/4.0/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "keywords"
    ]
   },
   "source": [
    "Multimodal, Annotation, Traces, Historiography, Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "abstract"
    ]
   },
   "source": [
    "The rise of born-digital materials and the digitization of existing archives is fundamentally changing the nature of sources that humanities researchers work with. As sources transform to multimodal digital traces from which we look to extract data it becomes necessary to also reflect upon the digital environments within which we work. Building upon a decade’s work of digital humanities research and software development, we present Arvest: a tool for interpretative manipulation of digital traces based on IIIF (International Image Interoperability Framework). After a comprehensive state of the art and establishing the theoretical framework that drives development, we shall present the technical specifications of Arvest, and how we intend it to simultaneously propose novel solutions to issues pertaining to working with digital traces and seamlessly integrate with existing tools. We shall also present a case study of research based on a multimodal corpus from the Horizon2020 COESO Project which drove Arvest’s initial development and illustrates some of its core functionalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the humanities, driven by the rise of born-digital materials and the digitization of existing archives, traces are becoming digital -making them a new type of primary source for historians. Digital technology fundamentally changes their ontology: becoming digital, traces have become fragmented, reduced to units of information, data. Digitizing a trace means discretization, making it suitable for calculation and manipulation. That is the fundamental distinction between the analog and the digital trace. Data from traces are no longer traces. They are “_capta_” <cite data-cite=\"6386835/JSS687WM\"></cite>. Historiography in a digital context is not a revival of quantitative history in the strict sense of the term. It calls first and foremost for practices, and in this practice, the narrative is framed. It is a question of practice that goes beyond simply exposing traces: we expose traces to manipulation, to modeling <cite data-cite=\"6386835/77JUQC9J\"></cite> <cite data-cite=\"6386835/NK8HP38Q\"></cite>, and theories and concepts to operationalization <cite data-cite=\"6386835/NSNHSUSL\"></cite>. Hence, a paradigm shift is taking place, inviting us to make history differently. Because traces are transformed, they invite a renewal of the critique of the sources we thought we knew and make research questions evolve, leading to other epistemologies. The digital turn revitalizes source criticism, methodological thinking, and narratives, moving beyond traditional methods of close reading to computationally assisted reading and contributing to the general reflection on digital history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, the digital traces of contemporary events are characterized by their number, multimodality, and fragility. As early as 2003, Roy Rosenzweig warned us: “_Historians, in fact, may be facing a fundamental paradigm shift from a culture of scarcity to a culture of abundance._” <cite data-cite=\"6386835/DEJEYRGC\"></cite>. The unprecedented increase in digital artifacts and data presents specific difficulties highlighted by several actors in digital history <cite data-cite=\"6386835/JD3GQRLV\"></cite> <cite data-cite=\"6386835/7JUQ27DG\"></cite> <cite data-cite=\"6386835/JI5V3DFF\"></cite> <cite data-cite=\"6386835/IN3DXMC2\"></cite>. Consider for example the digital archives of a performing arts company, which could contain thousands of files from a single production. These files, varying from images and videos to text and software, encapsulate elements of the performance, its creation, and its reception. The challenges in distinguishing between these different types of data highlight the complexity of working with digital archives. While some traces are easily associated with public reception or final performances, others, such as a rehearsal audio recording, defy simple categorization (is it merely a testimony of a rehearsal, a technical test, or a file used in performance?) and often require direct consultation with the creators for accurate interpretation. Despite the voluminous data available, and while they are of a greater variety and concern phenomena that were previously ‘untraceable’, many aspects remain under-documented. Not all traces can be collected from all collaborators. Certain stages of collaboration leave behind scant traces, such as a few photographs from a workshop or sporadic notes from offstage discussions. Certain parts of digital activity itself rarely result in preserved traces: those occurring on social networks, or during conversations on videoconferencing platforms or applications such as Slack. Lastly, digital traces are fragile: it is estimated today that the average lifespan and readability of a digital file is five years. In a few years, there is no guarantee that we will still be able to open and read certain files, especially those made in proprietary software The loss of traces is a real and inevitable phenomenon:  in the past few decades, the number of artists who have already lost not only traces but works due to digital erosion is far too high (see for example in media art <cite data-cite=\"6386835/3ILX8VAL\"></cite> and in music <cite data-cite=\"6386835/G36NI5EQ\"></cite>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the complex characteristics of digital traces, they nonetheless offer the promise of renewed studies. To continue in the context of performing arts studies, digital traces are a paradigm shift not only for the history of theater but also for creative process and aesthetic analysis. To fulfill these promises, we need to build specific tools and environments in order to handle and interpret digital traces in all of their subtleties and complexity. The development of such an environment reveals five primary challenges that must be overcome: to move away from a ‘silo’ approach and allow for the analysis of traces in a multimodal environment; to allow for both manual and automatic annotation of traces, including complex documents like video recordings; to allow for navigation between close reading and distant reading perspectives; to avoid losing context by linking data extraction and the original trace; to reintroduce narrative into visualizations in order to reconcile temporal and spatial representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These concerns have guided the development of Arvest, a multimodal environment for linking data and their context, annotating digital traces, and deriving narrative from data. While the starting point for this research and software is the analysis of corpora documenting the creation processes of performances, Arvest is aimed at many disciplines in the humanities and social sciences, especially historians. The goal of this tool is indeed to reconcile traditional hermeneutic approaches with methods from digital history. The application is still under development: as of the writing of this article, a [prototype](https://arvest.tetras-libre.fr/) is available for consultation online, with a first full release planned for the end of 2024. In this paper, we will first revisit the various lab-objects and co-design stages that led to its design. In a second part, we will detail the main epistemological and hermeneutic challenges as well as the technical choices and functionalities implemented. Finally, we will present a case study conducted with Arvest before concluding on future prospects within the framework of the ERC-funded STAGE project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing lab-objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major characteristics of digital traces is their experiential nature. To overcome the specific difficulties of digital traces, it is necessary to experiment with and on digital traces. Without action, some elements would not be observable, some objects would not be readable. That is why we rely not only on the methodologies and tools of digital humanities but also on action research by developing our own instruments. Arvest is the continuation of two previous projects, [Rekall](http://www.rekall.fr/) and [MemoRekall](https://memorekall.com/en/), both spaces for questioning themes, methodological tools, and environments in which we create lab-objects. To use Bruno Bachimont’s terms, the lab-object allows us to \"_avoid having to choose between an object restricted enough to allow its study and broad enough to observe the effects of its complexity. An artistic production taken as a lab-object presents a locality that neither dilutes nor dissolves its complexity, which remains observable for comprehensive study._\" The lab-object also allows for multidisciplinary work, and in the case of Rekall and MemoRekall, a co-design approach as well as an opening towards citizen science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rekall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, digital traces of works are numerous, and reading each trace is nearly impossible. In the context of performing arts studies, the question thus becomes: what information, what knowledge about the performances is it possible to extract from these traces? Which data should be prioritized to reconstruct a creative process? Contrary to the practice of creating a specific interface for each show, ANON has led a reflection on the design of software that can be applied to as many works as possible <cite data-cite=\"6386835/X2IS589E\"></cite> <cite data-cite=\"6386835/7R9S2WPF\"></cite>. She undertook this project in 2007 as part of the [DOCAM program](https://www.docam.ca) on \"Documentation and Conservation of Media Arts\" led by the Daniel Langlois Foundation in Montreal, and then worked in collaboration with [Buzzing Light](https://www.buzzinglight.com/) and Thierry Coduys for the development and design of a prototype for Rekall, starting in 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main objectives guided the design of Rekall: to help artists document their works so they can be reproduced and thus counteract the obsolescence of digital technologies; to assist researchers in studying the genetics of works in a context of born digital documents and big data. Yet, it is the same materials (traces of the creative process and the works) that are collected, analyzed, and visualized, according to different modalities and timelines. For artists, collection takes place during the creative process itself, progressively through the work stages; it is a living archive, in perpetual motion. The researcher intervenes once the process is completed, once the documents have somewhat stabilized and frozen, and they become traces of what took place. The goal is not to freeze the traces but, on the contrary, to foster a living documentation, which can be augmented, refined (based on contributions from artists, their teams, or researchers and curators) and also interpreted differently depending on research questions that are translated into visualization modes. The accumulation of traces is a key element for the effective operation of Rekall. Indeed, by analyzing these traces and correlating them with each other, Rekall gradually reveals characteristics specific to a given work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual practices of corpus processing separate documents by file types and work in sealed ‘silos’ (texts/images/sounds). Such treatment prevents us from obtaining an overall vision of the collected traces and examining many phenomena, such as the evolution of an idea through different traces, from an image to a text, from a text to a spreadsheet, from a spreadsheet to a video recording. To address this essential issue, Rekall was designed from the start <cite data-cite=\"6386835/7R9S2WPF\"></cite> as a multimodal environment that allows an infinite number of traces of different natures to be aggregated in the same interface. Rekall brings together texts, images, sounds, videos, programs, etc. in the same space. Trace analysis is based on the metadata present in each file that allows the extraction of crucial information (author, date of creation, place of creation, keyword, etc.). This information is then used by various data analysis and visualization algorithms to reveal behaviors and practices. By playing with axes and different filters that offer many different perspectives on a given creation, Rekall's functionalities allow for graphical analysis of the work through its traces. These different views allow navigation between the micro and the macro, between close reading and distant reading, between diachronic representation and synchronic representation. In other words, being able to represent a process without erasing its complexity. For a detailed description of the prototype and its interface, we refer to <cite data-cite=\"6386835/X2IS589E\"></cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MemoRekall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, Rekall was developed to manage documents within a multimodal framework, enabling the interconnection of various documents. Among these documents, video already held particular importance, with a very first attempt to build an interface in order to add video annotations and to allow comparisons. Video has become an indispensable tool for a variety of research in the humanities. For example, in the field of intangible cultural heritage, videos of cultural practices enable a meticulous study of gestures, expressions, and social interactions <cite data-cite=\"6386835/M25WPB3U\"></cite>. In sociology, recordings of focus groups or everyday life situations provide rich data on behaviors and social dynamics <cite data-cite=\"6386835/SLZ9AQ77\"></cite>.  Similarly, in linguistics, videos of verbal exchanges are essential for studying the nuances of non-verbal and paraverbal communication, such as body language, and facial expressions. In performing arts history, video recordings help to better understand the creative process <cite data-cite=\"6386835/8N72JUGA\"></cite> and build aesthetic analysis of performances <cite data-cite=\"6386835/MMEGRVTP\"></cite>. Videos indeed provide invaluable data, however they often need to be accompanied by expert interpretation and contextual information to be fully effective and explicit in conveying their intended insights. While video traces have spread across numerous fields and are used for a variety of purposes, they are rarely self-sufficient: to be explicit, they require commentary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the importance of video documentation into account, the video annotation functionality in Rekall was spun-off into MemoRekall: a separate, user-friendly web application designed for annotating videos and linking them to other documents or web pages. These annotations and links form a \"capsule,\" a new type of document that can be embedded in websites. Two complementary annotation strategies are allowed: inter-documentary (linking the video to a broader set of documents) and intra-documentary (annotating within the video itself). In the process of making the audiovisual document explicit by adding annotations and links to other documents, the video is ”redocumentarized” <cite data-cite=\"6386835/E8VRJPYX\"></cite>. In this way, MemoRekall makes it possible to reappropriate documentary corpora. From a single video, different documentary strategies can be developed, depending on the specific challenges of each capsule author: in the case of performing arts studies, artists need to document their works so that they can be easily distributed, adapted or reenacted; cultural institutions wish to share creative processes with their audiences, as well as elements of analysis on their websites; teachers need to present their students with different multimedia resources to accompany their courses on the history of the performing arts, or as part of educational activities linked to performances seen in class (please see an example capsule below, and for others visit [www.memorekall.com](https://memorekall.com/en/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"ratio\" style=\"--bs-aspect-ratio: 47%;\">\n",
    "<iframe src=\"https://project.memorekall.com/en/capsule/preview/hakanai-amcb\" webkitallowfullscreen=\"\" mozallowfullscreen=\"\" allowfullscreen=\"\" width=\"960\" height=\"422\" frameborder=\"0\"></iframe>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "### A co-design approach grounded in citizen science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "Such projects as Rekall and MemoRekall often fall somewhere on a spectrum between two approaches: on the one hand, the development of tools within the context of a specific research project that look to interrogate a specific corpus and that are designed to investigate specific epistemological questions; on the other, the development of more generic working environments which have a certain user profile in mind but do not base themselves around a specific use-case during development. In the former case, the tools that emerge can be characteristically project-specific, and can only be further applied in a restrained number of contexts; in the latter, tools runs the risk of allowing for functionalities and workflows that can appear useful in concept, but which crumble when applied to actual case-studies. This polarity illustrates the tensions that can arise when researchers and programmers meet to work in collaboration on a project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "This spectrum is a caricature – projects will inevitably find themselves somewhere along this gauge without reaching either of its extremities – but it is a useful idea to bear in mind when planning and executing projects that see academic, computer science and citizen profiles coming together to collaborate and create “_interdisciplinary trading zones_” <cite data-cite=\"6386835/HIGC4I2B\"></cite>. There are numerous examples in recent years where this balance would appear to have been achieved: for example the ERC-funded [FluCoMa project](https://www.flucoma.org/) <cite data-cite=\"6386835/6XP93NMB\"></cite> lead by Pierre Alexandre Tremblay at the University of Huddersfield saw computer scientists working closely with creative coders to bring a toolkit of machine learning algorithms and sound corpus manipulation tools to a number of creative coding environments; another is Taylor Arnold and Lauren Tilton’s [Distant Viewing Lab](https://distantviewing.org/) <cite data-cite=\"6386835/UZKX34LS\"></cite> at the University of Richmond which produced a python package <cite data-cite=\"6386835/348ACRBE\"></cite> for distant viewing in the digital humanities across a number of research projects with the intention of making their tool accessible to non techno-fluent researchers through a high-level command line interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "Following a design thinking methodology <cite data-cite=\"6386835/N7D5UUTW\"></cite>, Rekall and MemoRekall have emerged from a development process centered on collaboration and co-design. This approach aligns with the principles of citizen science, a practice where scientific research is conducted, in whole or in part, by amateur or nonprofessional scientists. Workshops have been carried-out in different contexts that were structured specifically to engage with users’ needs following the UK Design Council’s Double Diamond Framework <cite data-cite=\"6386835/3AYTRGVP\"></cite>: workshops were divided into exploration and development phases, where users first discover the tool and define their needs, then these needs are developed and delivered back to the users at which point the process can start again. Working in this way allows us to understand how the tools we have developed are understood by the users, meet interface and functionality needs that may have been opaque to the developers, and ground the tools in real use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "By involving a diverse group of stakeholders, including non-experts, in the research process, Rekall and MemoRekall not only benefit from a wide range of perspectives and expertise but also foster a sense of ownership and engagement among participants. For example, over the years, MemoRekall has been developed in the context of various citizen-science research projects, such as the Erasmus+ funded COSMIC project <cite data-cite=\"6386835/T4VBK6L6\"></cite> and the Horizon2020 funded COESO project <cite data-cite=\"6386835/VBD8BYQJ\"></cite>. In each instance, user feedback (professional circus teachers, choreographers, students from high school to MA) was meticulously considered and incorporated into the tool, addressing aspects ranging from interface design and user experience to specific functionalities and workflows. In the context of the COSMIC project, the application was tailored to the needs of circus education. This adaptation process involved close collaboration with circus educators and practitioners to identify and integrate features that support the complex and dynamic nature of circus training. Specific functionalities (slow motion, visual annotation) were developed to facilitate the detailed analysis of physical movements and the documentation of training routines. This collaborative work has yielded its fruits, and MemoRekall is now widely used by researchers, artists, cultural professionals, teachers and schoolchildren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From MemoRekall to Arvest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of the [COESO project](https://dansophie.hypotheses.org), the co-design approach led us to reconsider the video centric approach of MemoRekall and more braodly its development in the context of FAIR principles. During the work sessions on documenting the collaboration between a choreographer and dancer (Cosetta Graffione) and a philosopher (Stefania Ferrando), we concluded with Irénée Blin (Laban notator on the project) that video could no longer be the central document for annotation work. Instead, we needed to consider a network of multimodal documents that could annotate each other, as well as a means of changing perspective by opening documents that can be used as new entry points into the annotation network. Based on this observation, a researcher (ANON), a team of developers (ANON), a Laban notator (Irénée Blin) and a post-doc (ANON) worked together to improve the MemoRekall application and provide a prototype for a new version of the software: the development of a multimodal annotation editor to link a video recording to its Laban notation and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new conceptual approach and these new functionalities required a complete reimplementation of the tool. In order to base this reimplementation on open standards, the [W3C Web Annotation Guidelines](https://www.w3.org/TR/annotation-model/) provided an extensible data model and protocol for multimodal annotation across the World Wide Web. In addition, these W3C recommendations are endorsed by the [IIIF (International Image Interoperability Framework)](https://iiif.io/) initiative, which is receiving increasing attention from museums and other cultural institutions distributing digital content. For example, [Europeana](https://www.europeana.eu), which is a data source for the annotations initially targeted in the COESO project, uses IIIF to distribute its content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, in order to meet COESO's objectives, we decided to implement Memorekall-IIIF, a prototype for multimodal annotation that capitalizes on Memorekall's experience and communities, but is also based on IIIF's open standards. The tool is based on the IIIF [Mirador viewer](https://github.com/ProjectMirador/mirador) and can be extended via themes and plugins to suit specific community needs. The prototype, a COESO-based example and a sandbox are presented on this web page: [https://gitlab.tetras-libre.fr/iiif/POC-mirador](https://gitlab.tetras-libre.fr/iiif/POC-mirador). This prototype was the basis for what has now become Arvest. Its functionalities and the choice of the IIIF are described in the second part of this paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Video Annotation Tools: A Comparative Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first objectives of Arvest is the building of an effective video annotation tool for digital humanists based on the IIIF protocol and MemoRekall functionalities. In our study, we analyzed 51 video annotation tools focusing on multiple aspects such as supported storage services, annotation types, cooperation features, sharing capabilities, interoperability, extensibility, usage mode, business model, and governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "data-table",
     "table-1"
    ]
   },
   "source": [
    "| # |Software                                                               |Website                                                       |Supported storage/streaming services                                                                       |Annotation modalities                                                                                                                                                                                                                                                            |Cooperation features                                                                        |Sharing capabilities                                                                                                                                                                 |Interoperability                                                                                                       |Extensibility                                                                                                    |Local vs. online usage                                                                                     |Business model                                                                                                                               |Governance                                                       |\n",
    "|---|-----------------------------------------------------------------------|--------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------|\n",
    "||_The name of the software._|_The website where the software can be accessed._|_Where is the interrograted media stored (a service like Youtube, dedicated server, local machine etc.)._|_Ways in which the user can add annotations, types of annotations._ |_Ways in which the documents can be worked on by several people._|_Ways in which the documents can be shared._ |_Can the resulting documents and projects be opened in other programs, accessed by other frameworks. Openness of format._|_Is it possible to etend the environment (write plugins etc.)._ |_How can projects be sorted (cloud, local machine etc.)_|_Is the service free to use, open source, paid, subscription based etc._ |_What is the nature of the institution or body behind the program._|\n",
    "|1  |MemoRekall (legacy)                                                    |https://memorekall.com/en/                                    |Youtube and Vimeo.                                                                                         |Types of annotations: documents (pdf, image etc); text; urls. Annotations have metadata, can control the speed of video playback.                                                                                                                                                |Projects can have several authors.                                                          |Projects can be shared via a link, embedded in a website, or save to XML.                                                                                                            |Projects exported as XML file.                                                                                         |-                                                                                                                |Online storage with HumaNum. (XML files can be stored locally but no way of uploading downloaded XML files.|Open source, free to use. Account required.                                                                                                  |Project led by Clarisse Bardiot, Université Rennes 2.            |\n",
    "|2  |Videonotes                                                             |https://videonotes.net/                                       |Youtube.                                                                                                   |Type textual notes during playback, hit enter and the annotation will be created. A list is populated under the video.                                                                                                                                                           |-                                                                                           |-                                                                                                                                                                                    |-                                                                                                                      |-                                                                                                                |Online storage.                                                                                            |Free to use. Account required.                                                                                                               |-                                                                |\n",
    "|3  |VideoAnt                                                               |https://ant.umn.edu/                                          |Youtube.                                                                                                   |Add textual annotations. Each annotation is a conversation thread which can be responded to.                                                                                                                                                                                     |Users can edit the same project and respond to each other's annotations.                    |Projects can be shared via link, embedded in HTML and saved to various formats. Can create groups of users.                                                                          |Text, RSS, XML and JSON export formats.                                                                                |-                                                                                                                |Online storage. Projects can be downloaded but not uploaded.                                               |Free to use. Account required.                                                                                                               |University of Minnesota.                                         |\n",
    "|4  |Vibby                                                                  |https://www.vibby.com/                                        |Youtube.                                                                                                   |Select parts of the video and comment them with text. These can be subject to comment from other people. Work on multiple videos in a same project and the annotated fragments will constitute the final project.                                                                |Users can respond, comment and upvote the projects and annotations.                         |Share via the Vibby website, give the project a tag and other metadata. Alos share via link and embed in HTML.                                                                       |-                                                                                                                      |-                                                                                                                |Online storage.                                                                                            |Free to use. Account required.                                                                                                               |-                                                                |\n",
    "|5  |YiNote                                                                 |https://yinote.co/                                            |Any service with HTML video (Youtube, Vimeo, LinkedIn, Lynda, Coursera etc.).                              |A browser extension, add textual annotations during video playback. These can be interacted with to play the video from that moment. A gloabl overview page is also created and associated with a screenshot of the anntoation which can be edited (add shapes, text etc.).      |-                                                                                           |Export to JSON, PDF, markdown, google docs, everntoe etc.                                                                                                                            |Exportable to a number of different formats.                                                                           |Possibility to download and upload data files. The git repository is available for forking. Very well documented.|Online and local storage.                                                                                  |Free to use, open source. No account required.                                                                                               |Github community.                                                |\n",
    "|6  |Timelinely                                                             |https://www.timeline.ly/                                      |Youtube.                                                                                                   |Add timed annotations linked to images, google maps, text, urls, video and photo.                                                                                                                                                                                                |-                                                                                           |Share via a link.                                                                                                                                                                    |-                                                                                                                      |-                                                                                                                |Online storage.                                                                                            |Free to use, no account required.                                                                                                            |Small development team.                                          |\n",
    "|7  |Vizia - redirect to \"teachable.com\", depreciated.                      |-                                                             |Youtube.                                                                                                   |Create text, url, quizz and question annotations. Designed for data collection.                                                                                                                                                                                                  |-                                                                                           |Share projects with url.                                                                                                                                                             |-                                                                                                                      |-                                                                                                                |-                                                                                                          |-                                                                                                                                            |-                                                                |\n",
    "|8  |Verse                                                                  |https://www.verse.com/                                        |Upload your own video content (max 30Mo/month for basic account). Includes 360° video.                     |Create interactable projects with parts of the video which can be clicked on and using simple logic and decision points trigger other video playback. Annotations can be text, image and url.                                                                                    |-                                                                                           |Embeddable.                                                                                                                                                                          |-                                                                                                                      |API and plugin integration.                                                                                      |Online storage.                                                                                            |Free to use, account required. Premium accounts.                                                                                             |Professional development team (Bonza Interactive Group).         |\n",
    "|9 |Popcorn Maker - depreciated.                                           |https://foundation.mozilla.org/en/artifacts/popcorn-maker/    |HTML video.                                                                                                |Remix web video with text, image, ggole maps, wikipedia annotations using a layed timeline.                                                                                                                                                                                      |-                                                                                           |-                                                                                                                                                                                    |-                                                                                                                      |Built upon the popcorn.js library.                                                                               |-                                                                                                          |-                                                                                                                                            |Mozilla.                                                         |\n",
    "|10 |Videopath - depreciated.                                               |-                                                             |Youtube.                                                                                                   |Add text, image, url and audio annotations onto a vertical timeline.                                                                                                                                                                                                             |-                                                                                           |Embeddable.                                                                                                                                                                          |-                                                                                                                      |-                                                                                                                |-                                                                                                          |-                                                                                                                                            |-                                                                |\n",
    "|11 |Hihaho                                                                 |https://hihaho.com/                                           |Upload your own video, or upload from Youtube, Vimeo, JW Player, Qumu, Panopto, Mediasite or Blue Billiwig.|Create interactive videos using simple logic with button, menus, adding miages, text etc.                                                                                                                                                                                        |-                                                                                           |Embeddable, share via url.                                                                                                                                                           |-                                                                                                                      |Backend API.                                                                                                     |Online storage.                                                                                            |Free to use, account required. Premium accounts.                                                                                             |Professional development team.                                   |\n",
    "|12 |Edpuzzle                                                               |https://edpuzzle.com/                                         |Upload or record video. Alos Youtube and Vimeo.                                                            |Create pedagogical interactive videos - add quizzes, text, voice recordings etc.                                                                                                                                                                                                 |-                                                                                           |Integration with LMS services such as Google Classroom, Microsoft Teams, Canvas, Schoology, Moodle, Blackboard, Blackbaud, Powerschool, Clever and D2L. Create \"classes\" in Edpuzzle.|-                                                                                                                      |-                                                                                                                |Online storage.                                                                                            |Free to use, account required. Premium accounts for tearchers and schools.                                                                   |Professional development team.                                   |\n",
    "|13 |Mindstamp                                                              |https://mindstamp.io/                                         |Youtube, Vimeo, Wistia, Kaltura, Dropbox, Amazon S3, Cloudinary, JWPlayer.                                 |Create interactive videos with button, questions, drawings comments, images, audio and video, conditional logic, chapters etc.                                                                                                                                                   |-                                                                                           |Data analysis integrations with Hubspot, Salesforce, Zapier, Google Analytics, Segment, Constant Contact.                                                                            |-                                                                                                                      |Backend REST API and webhooks.                                                                                   |Online storage.                                                                                            |Free trial, but paid accoutns required.                                                                                                      |Professional development team.                                   |\n",
    "|14 |Advene                                                                 |http://www.advene.org/                                        |Local videos.                                                                                              |Downloadable program. Multiple interfaces for adding annotations. Define bookmarks, hyperlinks, files, text, SVG. Control video playback with annotations.                                                                                                                       |-                                                                                           |Export to multiple formats: SLIL, SVG, HTML+CSS, XML.                                                                                                                                |Export and import various formats.                                                                                     |Git repo and documentation available.                                                                            |Local storage.                                                                                             |Free to use, must be downloaded.                                                                                                             |LIRIS laboratory of University Claude Bernard Lyon 1.            |\n",
    "|15 |Celluloid                                                              |https://celluloid.huma-num.fr/                                |Youtube, Peertube.                                                                                         |Define metadata for the project. Create annotations by defining the time, if the video will pause, and a text.                                                                                                                                                                   |A project can have several members working on it.                                           |Embeddable and share via url.                                                                                                                                                        |-                                                                                                                      |Git repo available with documentation.                                                                           |Online storage.                                                                                            |Free to use, account required.                                                                                                               |Canevas consortium (Michaël Bourgatte and Laurent Tessier).      |\n",
    "|16 |Ksnip                                                                  |https://github.com/ksnip/ksnip                                |-                                                                                                          |Take screenshot and annotatate with various drawing tools. Possiblity also to blur and pixalate images.                                                                                                                                                                          |-                                                                                           |Integrated uploading to imgur. PDF and PS export.                                                                                                                                    |PS image export.                                                                                                       |Git repo available for forking.                                                                                  |Local storage.                                                                                             |Free to use, open source. No account required.                                                                                               |Github development community.                                    |\n",
    "|17 |Vialogues - depreciated (site down for maintenance at time of writing).|https://www.vialogues.com/                                    |Upload videos (1Go or less) or Youtube and Vimeo                                                           |Define a main questiona s a description, then users can add textual comments at given time points in the video. The user can also add polls as annotations.                                                                                                                      |Multiple users can interact with the project and add comments.                              |Url sharing and embeddable.                                                                                                                                                          |-                                                                                                                      |-                                                                                                                |Online storage.                                                                                            |Free to use, account required.                                                                                                               |EdLab at Columbia University.                                    |\n",
    "|18 |Motion Bank (Piecemaker/PM2GO)                                         |https://motionbank.org/                                       |Youtube, Vimeo.                                                                                            |Add timed textual annotations. Concept of \"timelines\", allowing for multiple videos in a same project.                                                                                                                                                                           |Possiblitiy to create groups and invite members to work on projects.                        |Sharing only seems possible between members.                                                                                                                                         |-                                                                                                                      |-                                                                                                                |Online storage.                                                                                            |Free to use, account required. Accounts created on request.                                                                                  |Mainz University of Applied Sciences.                            |\n",
    "|19 |Media Annotations Working Group                                        |https://www.w3.org/2008/WebVideo/Annotations/                 |Web video.                                                                                                 |Creation of an ontology and API designed to facilitate cross-community data integration of information related to media objects int he Web.                                                                                                                                      |-                                                                                           |-                                                                                                                                                                                    |This was the goal of the project.                                                                                      |All results are open source and adoptable.                                                                       |-                                                                                                          |Open source.                                                                                                                                 |Media Annotations Working Group.                                 |\n",
    "|20 |Dicto (Medialab)                                                       |https://medialab.sciencespo.fr/en/tools/dicto/                |Wen video and audio (Youtube, Vimeo, Soundcloud etc).                                                      |Create collections of documents as corpora. Segment the documents, give them metadata, comment them (notably designed to work for transcription).                                                                                                                                |-                                                                                           |Downloadable to various formats (including HTML webpage).                                                                                                                            |Download to HTML, json and various lists as tsv format.                                                                |Source code available on github.                                                                                 |Online and local storage.                                                                                  |Free to use. Can be downloaded as a local desktop application.                                                                               |Robin de Mourat and Donato Ricci.                                |\n",
    "|21 |IIIF: International Image Interoperability Framework                   |https://iiif.io/                                              |Web video.                                                                                                 |IIIF is a standard for interoperability and sharing digital artefacts. It is an API, a set of standards, and has various applications that can itnerpret it's data (Mirador, UniversalViewer etc.).                                                                              |-                                                                                           |-                                                                                                                                                                                    |This is the goal of the project.                                                                                       |Open source and available on github.                                                                             |-                                                                                                          |Open source.                                                                                                                                 |IIIF Consortium (65 institutional members).                      |\n",
    "|22 |Coach's Eye - depreciated.                                             |https://go.coachseye.com/retirement/                          |Local video.                                                                                               |Possibility to slow down video and draw annotations directly onto it (destined for coaches and athletes).                                                                                                                                                                        |-                                                                                           |Coaches can distribute projects to their team's devices.                                                                                                                             |-                                                                                                                      |-                                                                                                                |-                                                                                                          |Free to use.                                                                                                                                 |Professional development team (TechSmith)                        |\n",
    "|23 |MotionNotes                                                            |https://motion-notes.di.fct.unl.pt/                           |Local video, Youtube, Europeana, WeaveX.                                                                   |A layered timeline interface, add various types of annotations - drawing, text, voiceover, links, 3D objects. Also possible to control the speed of playback.                                                                                                                    |-                                                                                           |Embeddeable and url sharing.                                                                                                                                                         |-                                                                                                                      |-                                                                                                                |Online and local storage.                                                                                  |Free to use, account required.                                                                                                               |Universidade NOVA de Lisboa.                                     |\n",
    "|24 |Enhanced Unified Playout                                               |https://pro.europeana.eu/page/enhanced-unified-playout-service|Europeana videos.                                                                                          |Create segmentations and playlists of Europeana videos. Add annotations like text, subtitles, speech bubles to the video.                                                                                                                                                        |-                                                                                           |Create embeddable codeboxes.                                                                                                                                                         |Up to IIIF, W3C, HTML5 standards.                                                                                      |Source code available of github.                                                                                 |Online storage.                                                                                            |Free to use, account required.                                                                                                               |Europeana.                                                       |\n",
    "|25 |Playment                                                               |https://jarvis.playment.io/                                   |Local videos.                                                                                              |Create visual annotations for labelling content i n the video: 2D boxes, 3D cubes, point clouds etc. Used for building up models for ML algorithms.                                                                                                                              |Collaborative building of datasets.                                                         |-                                                                                                                                                                                    |-                                                                                                                      |-                                                                                                                |-                                                                                                          |Paid usage.                                                                                                                                  |Professional development team.                                   |\n",
    "|26 |KinoLab                                                                |https://kinolab.org/                                          |Local videos.                                                                                              |Upload videos to the platform and create labels and tags in order to create a large database open to researchers.                                                                                                                                                                |The platform is built up collaboratively with all users.                                    |-                                                                                                                                                                                    |-                                                                                                                      |-                                                                                                                |Online storage.                                                                                            |Free to use, account required.                                                                                                               |Bowdoin College.                                                 |\n",
    "|27 |Omeka                                                                  |https://omeka.org/                                            |Local videos.                                                                                              |Create collections in the style of media archives. A tool for building up virtual collections of archives, virtual visits etc.                                                                                                                                                   |-                                                                                           |Export to a nuble rof different format.                                                                                                                                              |Uses industry standards such as Dublin Core.                                                                           |Open source and source code available on github.                                                                 |Local storage.                                                                                             |Free to use, download required.                                                                                                              |Diogital Scholar (cf. Zotero).                                   |\n",
    "|28 |Mediate                                                                |https://rclmediate.lib.rochester.edu/                         |Local videos and audio.                                                                                    |Add annotations to content based on a \"Schema\", identifying specific content. Each note can be a thread that can be commented by other users.                                                                                                                                    |Define collaborators to work on the project.                                                |-                                                                                                                                                                                    |-                                                                                                                      |REST API.                                                                                                        |Online storage.                                                                                            |Free to use, account required.                                                                                                               |University of Rochester                                          |\n",
    "|29 |Semantic Annotation Tool                                               |https://mediaecology.dartmouth.edu/sat/                       |Web videos.                                                                                                |Create textual annotations with tags. It is the combination of two open source libraries: Waldorf.js and Statler. The project offers an end-to-end open source video annotation workflow designed to be incorporated into other projects.                                        |-                                                                                           |-                                                                                                                                                                                    |W3C Open Annotation spec.                                                                                              |Both Waldorf.js and Statler are open source and available on github.                                             |-                                                                                                          |Open source, free to use.                                                                                                                    |Media Ecology Project                                            |\n",
    "|30 |Annotate-On                                                            |https://www.recolnat.org/fr/annotate                          |Local images or video or images from Recolnat.                                                             |Various tools for adding visual highlights to an image, and also more bespoke tools like annotations which represent the counting of elements in an image.                                                                                                                       |Possibility to share a project across several machines.                                     |Share the project across machines. Export projects to CSV and IIIF to make available on Recolnat.                                                                                    |CSV and IIIF export.                                                                                                   |-                                                                                                                |Local storage and storage on Recolnat.                                                                     |Open source, free to use (must cite).                                                                                                        |Recolnat.                                                        |\n",
    "|31 |Altasciné                                                              |https://geomedialab.org/atlascine.html                        |Local video or audio is uploaded to the app.                                                               |Bespoke interface which needs a transcript of the audio or video file (designed to work with interviews). Link to the text fo the transcript tags and places on a map. This creates data that can be viewed in various perspectives.                                             |Possibility for several accounts to have acess to an Atlas.                                 |Share the link to the Atlas (can be password protected).                                                                                                                             |-                                                                                                                      |Well-documented on the git repo. Entire code base can be forked and deployed.                                    |Online storage.                                                                                            |Open source, free to use (account required).                                                                                                 |Geomedia Lab, Concordia University.                              |\n",
    "|32 |educARTE                                                               |https://educ.arte.tv/                                         |Arte videos.                                                                                               |Create network visulaizations of different types of documents: arte videos, PDFs, and links.                                                                                                                                                                                     |Embedded within the French school system. Projects can be viewed by teachers and classmates.|Share projects within the educarte system.                                                                                                                                           |-                                                                                                                      |-                                                                                                                |Online storage.                                                                                            |Free to test. Contracts are made available to educational institutions.                                                                      |Small development team.                                          |\n",
    "|33 |oTranscribe                                                            |https://otranscribe.com/                                      |Local audio or video files. Youtube.                                                                       |Tool for helping with transcription. Create a text document while watching the video. Keyboard shortcuts allow for playback control. Video speed control.                                                                                                                        |Share files directly on Google Drive.                                                       |-                                                                                                                                                                                    |Export and import of markdown and plain text.                                                                          |-                                                                                                                |Online usage, offline storage.                                                                             |Free to use.                                                                                                                                 |Single developper for the MuckRock foundation.                   |\n",
    "|34 |sonal                                                                  |http://www.sonal-info.com/                                    |Local video and audio.                                                                                     |Perform segmentations, and add textual annotations in a layed timeline. Augment transcriptions with speaker attribution. Text formatting (bold, italic etc.). Give tags to different segments. Basic NLP analyses and data-driven interfaces dervied from this data.             |-                                                                                           |-                                                                                                                                                                                    |Import and export various text formats. Windows XP and 8 only.                                                         |Code not available.                                                                                              |Local usage.                                                                                               |Free to download and use.                                                                                                                    |CAQDAS. 2 developers.                                            |\n",
    "|35 |autoEdit                                                               |https://opennewslabs.github.io/autoEdit_2/                    |Local video.                                                                                               |Tool for speech-to-text transcription. Add a video, then choose a speech-to-text algorithm. The transcribtion is timbe-linked.                                                                                                                                                   |-                                                                                           |-                                                                                                                                                                                    |Export as EDL, or srt format. Can also export to bespoke video editors.                                                |Free and open source, code is available on github.                                                               |Local usage.                                                                                               |Free and open source.                                                                                                                        |OpenNews Labs.                                                   |\n",
    "|36 |FrameTrail                                                             |https://frametrail.org/                                       |HTML5 video.                                                                                               |Place documets on top of the video (text, image, web pages, interactive maps). You can also add javascript code snippets to be executed at certain points of the video. Content can be viewed in-time or as non-linear networks of video fragments which can be navigated freely.|Compare your project with the annotation timelines of other users.                          |-                                                                                                                                                                                    |Proprietary format only.                                                                                               |Open source, code is available on github and forking is encouraged.                                              |Local usage (must be run on a local web server).                                                           |Free and open source.                                                                                                                        |Merz Akademie, Stuttgart.                                        |\n",
    "|37 |scenari                                                                |https://scenari.software/fr/                                  |Local video and audio.                                                                                     |A tool for creating textual content that can be augmented in various ways (adding video, audio, image, quizz etc.). The content can then be exported in various formats (web, pdf, xml, ePub etc.).                                                                              |Integrated cooperation tools with solutions for todo lists, proof-reading workflow etc.).   |Export projects to a number of different formats (web page, pdf, xml, ePub etc.).                                                                                                    |Multiple export functionalities lead to interoperability.                                                              |Designed with extensibility in mind, possibility to edit the ways in which are exported.                         |Local storage (or online if you use the client-server verison of the tool).                                |Free to use.                                                                                                                                 |Kelis.                                                           |\n",
    "|38 |Entity Mapper                                                          |http://piim.newschool.edu/entitymapper/#!/home                |-                                                                                                          |Visualize ATLAS.ti format data with different visualizations, notably network visualisations.                                                                                                                                                                                    |-                                                                                           |-                                                                                                                                                                                    |-                                                                                                                      |Open source and available on github.                                                                             |Local usage (download required).                                                                           |Free to use and open source.                                                                                                                 |The Parsons Institute for Information Mapping.                   |\n",
    "|39 |Lignes de temps - depreciated                                          |https://www.iri.centrepompidou.fr/outils/lignes-de-temps/     |Local videos , Youtube, URL.                                                                               |Create segments for the video which have a number of metadata (title, tags, description, colour etc.). You can also add an audio file to a segment.                                                                                                                              |-                                                                                           |Publish the project within the IRI community.                                                                                                                                        |-                                                                                                                      |-                                                                                                                |-                                                                                                          |Free to use (account required).                                                                                                              |Institut de Recherche et d'Innovation                            |\n",
    "|40 |eTalks                                                                 |https://etalk.vital-it.ch/                                    |Local audio and image.                                                                                     |Create interactive presentations of slides based on three components: a text, an audio recording of the text, and image.                                                                                                                                                         |-                                                                                           |Share via direct link or HTML embed code.                                                                                                                                            |-                                                                                                                      |Available open source on github.                                                                                 |Online storage.                                                                                            |Open source, free to use.                                                                                                                    |Swiss Institute of Bioinformatics                                |\n",
    "|41 |Videogrep                                                              |http://antiboredom.github.io/videogrep/                       |Local video.                                                                                               |A command line tool (that can be used in python) for create composite videos from subtitle files and textual transcriptions.                                                                                                                                                     |-                                                                                           |-                                                                                                                                                                                    |Export to various video formats, including bespoke video editors.                                                      |Open source and available on github.                                                                             |Local storage.                                                                                             |Open source, free to use.                                                                                                                    |Single developer (Sam Mavigne).                                  |\n",
    "|42 |Speech Editor                                                          |http://ucbvislab.github.io/speecheditor/                      |Local audio.                                                                                               |Interactively edit audio from text transcriptions (cut audio, add pauses breaths etc).                                                                                                                                                                                           |-                                                                                           |-                                                                                                                                                                                    |-                                                                                                                      |Open source and available on github.                                                                             |Local storage.                                                                                             |Open source, free to use.                                                                                                                    |UC Berkely.                                                      |\n",
    "|43 |InterLace                                                              |https://github.com/strob/interlace                            |HTML5 video.                                                                                               |A node module for creating interactive timelines of videos with limited textual annotation. The tool renders zoomable timelines that give a preview of each frame, allowing the user to gain a comprehensive image of the video contents.                                        |-                                                                                           |Export to a viewable only format.                                                                                                                                                    |-                                                                                                                      |Open source and available on github.                                                                             |Local usage.                                                                                               |Open source, free to use.                                                                                                                    |Single developer (Robert M Ochshorn) at Jan van Eyck Academie.   |\n",
    "|44 |F5                                                                     |https://macdownload.informer.com/f5-transcription-free/       |Local video and audio.                                                                                     |A transcription tool with variable media playback speed, timestamps and speaker tokens.                                                                                                                                                                                          |-                                                                                           |-                                                                                                                                                                                    |-                                                                                                                      |-                                                                                                                |Local usage.                                                                                               |Free to download.                                                                                                                            |Dr. Dresing & Pehl GmbH.                                         |\n",
    "|45 |Descript                                                               |https://www.descript.com/                                     |Local video and audio.                                                                                     |A tool for creating videos based on transcription similar to speech editor. Remove filler words, perform audio and video editing manipulations. Annotate video with backgrounds and inserting images.                                                                            |-                                                                                           |Can publish interactive transcripts to the web and allow for commenting. Share direct link or embeddable HTML.                                                                       |-                                                                                                                      |-                                                                                                                |Local usage.                                                                                               |Free trial with paid premium options.                                                                                                        |100+ team.                                                       |\n",
    "|46 |Open Parliament TV                                                     |https://openparliament.tv/?lang=en                            |-                                                                                                          |A project that provides an interface for navigating parliamentary debates. Videos are coupled with interactive transcripts, and annotated with links to documents when necessary.                                                                                                |-                                                                                           |Allows for citation of fragments of parliamentary debates.                                                                                                                           |Access data via API calls.                                                                                             |Open source and available on github.                                                                             |Online usage.                                                                                              |Open source, everything is available on github.                                                                                              |Open parliament TV (development team, size unknown).             |\n",
    "|47 |Dspace                                                                 |https://www.4science.com/dspace-glam/                         |Local media content.                                                                                       |Dspace is a suite of different tools for digital asset management and online dissemination. It functions with a number of add-ons, notably a IIIF Image viewer and OCR tools for manuscript analysis.                                                                            |Dependent on add-on.                                                                        |One of the primary goals of Dspace is for digital dissemination of content in various forms.                                                                                         |Some of the outputs are interoperable, for example IIIF.                                                               |-                                                                                                                |Local and online usage.                                                                                    |Based on open source technologies. However, no available download, you must pay to have Dspace set up the systems your isntitutions requires.|Space, a large devleopment company.                              |\n",
    "|48 |Prezi                                                                  |https://prezi.com/                                            |Local media.                                                                                               |Create interactive presentations, much like powerpoint, but online. Possibility to include various media formats. Presentations can be based on video with animated images annotating the main ressource.                                                                        |Possibility to share presentations between accounts and reuse presentations.                |Share as embeddable code on the web or direct link.                                                                                                                                  |-                                                                                                                      |-                                                                                                                |Online usage.                                                                                              |Free trial , then paid usage.                                                                                                                |Large developper team.                                           |\n",
    "|49 |Loom                                                                   |https://www.loom.com/fr                                       |In-app screen capture.                                                                                     |Annotate screen captures in various ways, notably insterting images onto the screen.                                                                                                                                                                                             |Projects are shared amoungst teams, and there is support for commenting, reactions etc.     |Share with various permissions, share to social media, embed codes.                                                                                                                  |-                                                                                                                      |-                                                                                                                |Online usage.                                                                                              |Free trial, then paid usgae.                                                                                                                 |Large developper team.                                           |\n",
    "|50 |TIB AV-Analytics                                                                   |https://av.tib.eu/                                       |Local video is uploaded and stored on the platform.                                                                                     |Various machine learning-driven analyses (shot-detection, scene recognition etc), the results of which get displayed on different timelines. Timelines can also be created manually.                                                                                                                                                                                             |Videos can be shared with other users.     |-                                                                                                                 |Export ML-derived data to various formats.                                                                                                                      |Certain parts of the code are available of github.                                                                                                               |Online storage                                                                                             |Free to use                                                                                                                 |Deutsche Forschungsgemeinschaft – German Research Foundation (DFG). Small research team.                                           |\n",
    "|51 |CLARIAH Media Suite                                                                   |https://mediasuite.clariah.nl/                                       |Exploit data from a number of Dutch audiovisual archives.                                                                                     |The palform offers a number of tool for distant reading, searching etc.                                                                                                                                                                                             |Projects can be shared between users.     |-                                                                                                                 |The tool can produce raw data which can be exploited in any number of ways.                                                                                                                      |Projects are driven by jupyter notebooks, thus are inherently extensible.                                                                                                               |Cloud and local storage.                                                                                             |Free to use for people detaining certain university credentials.                                                                                                                 |CLARIAH research infrastructure, large research team.                                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the video annotation tools analyzed support YouTube as the primary storage service. Tools like MemoRekall and Hihaho also support other services such as Vimeo. Additionally, some software like Advene and Omeka allow for local storage, providing extra flexibility for users who prefer to work offline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textual annotations are the most common among the tools studied. However, some software goes beyond by offering advanced annotations such as inserting images, URLs, and interactive elements. For instance, Timelinely allows timed annotations linked to images, Google maps, text, and URLs. Edpuzzle stands out by enabling the addition of quizzes, text, and voice recordings, making it a comprehensive educational tool for creating interactive videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaboration is an important feature for many tools. MemoRekall and VideoAnt enable multiple users to work on the same project, facilitating team collaboration and co-creation of annotated content. However, other tools do not offer specific cooperation features, which can limit their use in collaborative contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most tools offer sharing options via direct links or HTML embedding. VideoAnt, for example, allows projects to be shared via links, embedded in HTML pages, and exported in various formats. YiNote is notable for its export capabilities to multiple formats such as JSON, PDF, and markdown, making it easier to share and integrate annotations across different platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interoperability is an area where many tools have limitations. A few tools, such as YiNote and Mindstamp, allow export to standardized formats or integration with other platforms. \n",
    "Extensibility through plugins or APIs is not widely available but is present in some tools like Verse and Hihaho, offering users the ability to customize and enrich the core functionalities according to their specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools also vary in terms of usage mode. Some are strictly online, such as FrameTrail and Loom, while others, like autoEdit and Entity Mapper, can be used locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of tools are free to use, although some require account creation. Others offer premium features or are entirely paid services, like Mindstamp and Dspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools are developed by a variety of entities, ranging from academic institutions, such as MemoRekall (Université Rennes 2) and VideoAnt (University of Minnesota), to professional development teams and open-source communities. This diversity in governance often influences development priorities, support quality, and the sustainability of the tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arvest, an environment to interpret digital traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The survey on video annotation tools, although not exhaustive, reveals that no existing solutions fully meet all the requirements necessary for humanities research. On a practical level, these requirements include: storage services such as YouTube, Vimeo and PeerTube, as well as public and private video facilities; a wide range of annotation possibilities (aside the document or as overlays, textual and visual annotations, and the ability to link to web pages and documents); collaboration features; sharing options on the web and import/export in various formats (CSV, JSON, XML); interoperability and, more generally, adherence to FAIR principles; extensibility through plugins or APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a more general level, the passage from MemoRekall to Arvest shows how difficult digital traces are to interpret and thus the necessity of an “_interpretative interface_”, which Johanna Drucker describes as supporting “_acts of interpretation rather than simply returning selected results from a pre-existing data set._” It “_will orchestrate_ […] _the shift from conceptions of interface as things and entities to that of an event-space of interpretative activity_”. She concludes: “_More attention to acts of producing and less emphasis on product, the creation of an interface that is meant to expose and support the activity of interpretation, rather than to display finished forms, would be a good starting place_” <cite data-cite=\"6386835/MTWML2AM\"></cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the journey of building such an interpretative interface, we moved in the context of the COESO project (see below) from video annotation to the interpretation of digital traces. The video-centric approach of MemoRekall is still possible, however we also offer a fundamental shift in perspective – that of a network of multimodal documents, where any artifact of any kind may become the focus of one’s project. The main research questions leading the development of this new version, named Arvest, are:\n",
    "\n",
    "- How can software actively support and enrich a semantic line of interpretation combining qualitative and quantitative approaches around a collection of multimodal documents?\n",
    "- How can software create research artifacts that participate in the researcher’s analysis?\n",
    "- How can software allow for the creation of “interpretative interfaces” that facilitate research and actively participate in analysis?\n",
    "- How can we link multimodal documents and data extracted from them in the same environment in order to keep the context of the data and to go back and forth between close and distant readings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction of digital traces to data is not neutral: it is fragmentation, reduction, even decontextualization. In this context, it is essential, as Bruno Latour's text on the construction of scientific reference <cite data-cite=\"6386835/MJDE3VNJ\"></cite> shows, to document all stages that lead from the work and its creative process to the trace, from the trace to the data, from the data to its visualization and interpretation; and to understand the implications of each stage in this chain in terms of data interpretation and trace reduction. In other words, to be able to go back to the source, to identify the various manipulations and instrumentations that led to a given visualization or interpretation. Each step leads to an interpretation, which must be documented as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development theoretical framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build a framework for development, we identified five main structuring challenges to overcome which we detail in the following lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing traces in a multimodal environment and moving away from a silo approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into account the multimodal features of the documents we are confronted with as historians is considered an important turn within the general context of digital humanities <cite data-cite=\"6386835/TYGJBIQA\"></cite>, and more specifically in the context of computational humanities research <cite data-cite=\"6386835/ZM8SK4WQ\"></cite>. If \"_multimodal_\" seems to have replaced the term \"_multimedia_\" in recent times, it's not merely to update the vocabulary. The shift emphasizes the simultaneous connections and interactions between different media—such as text, images, sound, and video—rather than their simple juxtaposition. According to the Oxford English Dictionary, \"_multimedia_\" pertains to creative works that utilize or combine various forms of digital content, such as text, audio, video, and animation - the emphasis here is placed on the medium that supports the content. Conversely, \"_multimodal_\" refers to the incorporation or utilization of several different methods or systems to convey a message, integrating multiple semiotic modes of communication—such as text, images, motion, and audio—within a single artifact. Here we are dealing with a mode, a form, rather than a supporting medium. \"_Multimodal_\" underscores the inclusion of different modes of communication that cannot be superimposed with a media, such as movement or gesture in videos. As Bateman elucidates, \"_nowadays that text is just one strand in a complex presentational form that seamlessly incorporates visual aspects ‘around’, and sometimes even instead of, the text itself._\" This underscores that multimodal projects go beyond merely juxtaposing different media formats; they involve the orchestrated interaction of these modes to create a cohesive presentation. Bateman further emphasizes this by stating, \"_Combining these modes within a single artifact—in the case of print, by binding, stapling, or folding or, for online media, by ‘linking’ with varieties of hyperlinks—brings our main object of study to life: the multimodal document._\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of digital humanities, recognizing the multimodal nature of documents is crucial. Traditional siloed approaches often separate documents by file types—text, image, etc.—which limits a comprehensive analysis of the interconnectedness of different elements, especially in image and video analysis. The multimodal approach aims to integrate these different modes into a cohesive whole, highlighting how they work together to convey meaning. This broader perspective allows for a more comprehensive and dynamic understanding of how various semantic elements interact within a single artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually and automatically annotating traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether through annotating video recordings or enriching textual data, digital technologies enable a dynamic interaction with semiotic content. Annotation, as defined by Manuel Zacklad, refers to \"_any form of addition intended to enrich an inscription or recording in order to attract the receiver’s attention to a passage or to complete the semiotic content by relating it to other pre-existing semiotic content or by an original contribution_\" <cite data-cite=\"6386835/E8VRJPYX\"></cite>. One of its main goals is to make the implicit explicit. Annotation is most of the time considered a private practice, for example in the first steps of a research process to mark important passages of a document, to highlight specific words or visual elements, or to track and grasp an emerging analysis. It can also be part of a research group practice, to collectively annotate a collection of documents according to specific needs and rules, as we see for example with crowdsourcing. But these annotations can also move to the public sphere: for example if we consider footnotes as an annotation practice, or if we decide to publish and edit private annotations. In computer science, openly publishing the annotations which led to the training of a machine learning algorithm is also a practice encouraged by open science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, annotation practices have evolved significantly with the advent of digital technologies, expanding the possibilities of annotation beyond traditional textual forms to include for example graphic and audiovisual annotations. Digital annotation practices extend also to the concept of metadata, which play a crucial role in describing the content and characteristics of digital files. Metadata is a useful tool in many contexts, however it does present some limitations in regard to the challenge of adequately capturing the semantic richness contained within  files. To address this, more advanced tools such as image recognition, topic modeling, and lexicometry are being explored in the digital humanities to extract more precise information from visual and textual data. Hence, to the private/public dichotomy of annotations, we also can add a manual/automatic one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on the afore-discussed experience with MemoRekall, being able to move from private to public annotation, as well as integrating manual and automatic annotations is one of the challenges we wish to address with Arvest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Navigating between close reading and distant reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many research projects involve corpora with a small number of documents that need to be annotated manually. That's why we feel it's important to offer an interface that can be used to annotate small corpora, as well as larger corpora requiring automatic annotation or a combination of manual and automatic approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integration of annotation tools offers a more nuanced understanding of digital content, bridging the gap between close reading and distant reading. Close reading, the traditional method used by SHS researchers, involves meticulously deciphering the content of a document. In contrast, distant reading, theorized by Franco Moretti, involves analyzing large literary corpora by observing relationships between texts rather than reading each one closely. Distant reading allows researchers to move \"_from texts to models_\", identifying patterns and relationships across a vast number of documents <cite data-cite=\"6386835/Q5RA5AZ4\"></cite>. Similarly, distant viewing, as defined by Taylor Arnold and Lauren Tilton <cite data-cite=\"6386835/3QQEILFY\"></cite>, extends this concept to corpora of still or moving images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to create a continuum that enhances our interpretative capabilities and supports the development of more sophisticated data visualization interfaces. This multi-scale approach allows researchers to toggle between detailed analysis and broader pattern recognition, ultimately fostering a deeper and more comprehensive understanding of digital content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoiding loss of context by linking data extraction and the original trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue of the link between data and its original context — its source — is today a major concern. To really grasp the scale of this problem and its cultural embededness, one may consider for example the marked absence of any cited source in the results of queries made with digital tools such as chatGPT. In history, the issue is crucial and at the very heart of the historian's practice: the practice of footnotes is there to link the stated fact and its source, preserved and accessible in a specific archive collection. \"_Citing one's sources_\" and \"_going back to the source_\" is the creed taught from generation to generation <cite data-cite=\"6386835/J938J5MK\"></cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a digital context, this usage translates into the description of the methodology, the process that goes from sources to data and then to their visualization and interpretation. Such \"_good practices_\" are at the heart of FAIR recommendations and the reproducibility of research. Thus, today, in conferences and publications dedicated to digital humanities, many presentations and articles include a presentation of the workflow that led from sources to their interpretation <cite data-cite=\"6386835/SQPWDX9P\"></cite>. The development of data papers follows the same direction. Yet, it is often difficult, for example at the time of data visualization, to go back to the source with a single click, as one might do with a footnote. It can be assumed that this problem will only grow with the availability of \"data sets\" that are more or less ready to use but cut off from their original sources. The problem is far from being ignored and there have been multiple initiatives by developers of document exploration interfaces  and digital publications to reconcile the data and the source from which it is extracted. One notable example is the Impresso project, Media Monitoring of the Past, which allows moving from different visualizations of data related to subjects, people, or places mentioned in the texts and images of a multilingual newspaper corpus, to viewing and reading the source from which these data were extracted <cite data-cite=\"6386835/YDW75SLU\"></cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it must be acknowledged that a gap is widening at the moment of the transition from unstructured data (the sources) to structured data. This transition, the operation of extracting data from documents, can take place manually (by entering information into a spreadsheet) or computationally (for example, with machine learning or deep learning processes). By transforming the trace into data, the researcher performs a triple operation of simplification, reduction, and construction of the information available in the digital trace: simplification, because not all the complexity of a digital trace can be taken into account; reduction, because only a limited number of variables are considered; construction, because the preceding operations are deliberate actions carried out in function of specific research questions and objectives. To be able to perceive the connections between traces through their fragments, it is necessary to produce an image (whether visual or auditory) that presents a synthetic vision of the data and their relationships. Taken separately, the data are devoid of meaning. They are mere noise, a confusing mess from which one must grasp \"_the overall sense of an obscured reality_\" <cite data-cite=\"6386835/L4J5MYTK\"></cite>. By accumulating them within an image, by recreating continuity between discontinuous elements, by replacing isolated elements within a whole, it is then possible to make them intelligible. If digital traces need to be instrumented to be read and analyzed, the question remains how, and at what cost? Data consists of a minimal unit of information that is both simpler to preserve and to process in comparison with its unstructured trace. But this is done at the cost of cutting the traces into an infinity of fragments. The risk is the loss of the overall view of the trace and the context from which the data was extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hiatus between the trace and the data, the break in causality caused by capturing and coding, necessitates the creation of multimodal environments for research, particularly in history, which allow for reconnecting and restoring continuity between the data and the trace from which it was extracted, as easily and directly as using a footnote, or even more effectively. While footnotes often remain theoretical due to the logistical challenges that physical archive visits pose for the reader, our goal is to make this potential accessible by offering the possibility to directly consult the referenced sources. In this perspective, \"_Data in Context_\" could become the guiding principle of digital historiography, the digital equivalent of the traditional \"_cite your sources_\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reintroducing narrative into visualizations in order to reconcile temporal and spatial representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integration of narrative into visualizations plays a pivotal role in reconciling temporal and spatial representations. Narrative, encompassing both time and space, transforms how we perceive and interact with data, allowing us to weave together disparate elements into a coherent whole. As Alberto Cairo <cite data-cite=\"6386835/9BB8NEPZ\"></cite> suggests, visualizations can be utilized at various stages of the research process: during exploration to derive new insights, in discovery and interpretation to uncover hidden patterns, and as an end product for disseminating research findings. This multipurpose use underscores the necessity of incorporating narrative to link complex documents, which exist not in isolation but as interconnected pieces of a larger collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paradox of aggregation lies in its inherent fragmentation. While aggregation strives for completeness, it merely captures a fragment of the vast information available on the Web or within large databases. The real challenge lies within the need to increase content but to also simultaneously maintain a human scale, allowing the collection to be perceptible as a whole, in each of its elements, and in the connections between them. The interplay of the whole and the part, and the part and the whole, necessitates the reconstruction of a narrative that links these fragments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In digital historiography, two types of narratives emerge: the narrative of the process steps and the narrative of the process results. The connotation step implied by the analysis of a data visualization serves as a narrative, reintroducing time into what primarily appears as static spatial representations. Contemporary interfaces rarely support dynamic representation, presenting us instead with computer-generated images that offer an overview of calculated results. Connotation becomes an unfolding operation, akin to carefully spreading out a crumpled piece of fabric. The narrative reintroduces hermeneutics and esthesis, countering the desire for a purely mathematical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narrative, therefore, is not merely a discourse of proof. Despite the allure of measurement in certain projects, calculation alone lacks objective criteria. Meaning emerges through successive interpretations and instrumentations, necessitating a narrative to explain the process and propose a reading of its results. The paradigm remains one of interpretation, not explanation. By reintroducing narrative into visualizations, we bridge the gap between temporal and spatial representations, fostering a deeper understanding and appreciation of the data we seek to analyze and interpret. This narrative-driven approach ensures that visualizations are not static endpoints but dynamic tools for continuous discovery and storytelling in the digital age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "### Specs and prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "We shall now give a brief overview of the specifications and functionalities of Arvest as a tool. As mentioned above, we made the choice to make Arvest fully compatible with IIIF. Even if this aspect will not necessarily be exposed to the average user, it is an important part of the tool and as such we shall now quickly go over some of the main advantages of IIIF and how they propulse Arvest as a platform. Technically, IIIF is comprised of two main parts: a set of open APIs that each have their own functionalities (An Image API for serving high resolution images; a Presentation API for serving media along with other elements like metadata and annotations; ab Authorization Flow API for handling access control; a Content Search API for searching through textual annotations; and a Content State API for serving parts of a IIIF Manifest in a compact format); and a number of open source and proprietary IIIF-compatible viewers that can serve media. Let us briefly focus on the [Presentation API](https://iiif.io/api/presentation/3.0/ ), which is currently in its third version. This part of the specification allows for the creation of a IIIF _Manifest_, which is a JSON format document which, using the W3C Web Annotations model, refers to a media resource object (or range of objects) as well as other elements such as metadata, annotations, required statements etc. The IIIF Manifest is a versatile tool that has several advantages:\n",
    "\n",
    "- It is _lightweight_. As a textual JSON document, stocking and exchanging IIIF Manifests requires very little resources.\n",
    "- It is a _referential_ document. Each element in a Manifest can point to a URL which means that it is possible to remove the need for duplicate uploads of data and media. This is also useful in an academic context, where parts of the documents or the entire document can be precisely cited and perpetuated with DOIs.\n",
    "- It is _interoperable_. All IIIF viewers must be able to interpret a IIIF Manifest, ensuring that content within the Manifest will always be accessible regardless of the technology used to open it.\n",
    "- It is a _composite_ document. With a IIIF Manifest, it is entirely possible to encapsulate within one lightweight, interoperable file all of the complementary elements like metadata and annotations we have discussed in this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "To these advantages, we can also add the fact that hundreds of collections from various institutions around the world have already started making their content IIIF-compliant - meaning that users in a IIIF-driven environment already have access to millions of different digital objects from around the world. The majority of these objects are images (as the name suggests, IIIF was initially conceived to serve images), however the third version of the Presentation API now allows time-based media to be included. Many of the IIIF viewers however have not yet incorporated this functionality. This is where the work we are doing will also allow us to contribute to IIIF’s community of open source developers - our time-based media serving and annotation tools will be fully available on their own for the community to use as they wish independent of the Arvest environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "As such, Arvest started life as a _fork_ of the widely-used IIIF viewer Mirador. We added [video and audio playback functionalities](https://github.com/SCENE-CE/mirador-video), and also developed a [plugin](https://github.com/SCENE-CE/mirador-annotation-editor-video) for image and audiovisual resource annotation. Building on the annotation functionalities of MemoRekall, the user can configure an annotation in the following way:\n",
    "\n",
    "- Annotation _metadata_. An annotation can have any number of metadata attached to it (typical fields include _title_, _author_, _tags_ etc.). By default, all metadata will be Dublin Core compliant, however we wish to enable any kind of ontological model. Every annotation can also target a specific region of the document in question, be it in the spatial or temporal dimensions. When linked to a region, a certain degree of interactivity with the document can be enabled, for example clicking on an annotation will jump playback of a time-based media to the beginning of the annotation’s start time.\n",
    "- Annotation _content_. As well as metadata, we can add content to an annotation. This can go from simple textual description augmented with html tags, to overlays that will be displayed on top of the related media. This content can be freely created and edited within Arvest.\n",
    "- Linking to _external documents_. Following the inter-documentary approach to annotation, it is also possible to link to other documents. Like in MemoRekall, it is possible to link to static files or links on the web, but a powerful addition is the ability to link an annotation to another IIIF Manifest. When this is the case, Arvest will allow the user to directly open the Manifest within the environment, and thus fluidly navigate a network of IIIF Manifest. This functionality can also be used to make data-driven interfaces. For example, nodes on an image representing a network of documents can be annotated so that clicking on a node in the image will directly open the document in Arvest - thus allowing a rapid switch between distant and close reading perspectives. We demonstrate the power of this kind of interface in the third part of this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "All of these Manifest editing functionalities will be wrapped within a [multiuser environment](https://github.com/SCENE-CE/mirador-multi-user), written in [Django](https://www.djangoproject.com/). This comes in answer to several aspects which we found could be significant hurdles to overcome for newcomers to IIIF, especially individual users with low techno-fluency:\n",
    "\n",
    "- _Connect and play_. Using IIIF today can be complicated - often users will find themselves cloning git repos, running servers from the command line, editing JSON etc. Another alternative is to have an external provider come and set up a working environment for you or your institution - naturally this is not an option in many cases. Arvest will allow users to simply create a free account, connect to the web application and start using IIIF without even having to know what it is.\n",
    "- _Media storage_. Each user will have a certain amount of storage space where they will be able to stock media and Manifests. We will also provide an instance of [PeerTube](https://joinpeertube.org/) for the handling of large video files. Users will have full control over their content, meaning that they can expose their IIIF-compliant content to the world, or keep everything private.\n",
    "- _Collaborative work_. Users can share their projects between each other, and also share them as embeddable html elements. Users retain full access control of their projects, allowing them to be edited or not by other users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "As mentioned above, the user can consult a [live demo](https://arvest.tetras-libre.fr/) of the Arvest environment that at the time of writing allows them to navigate an example project (see Section 3) and also a sandbox to create their own non-persistent content. It should be noted that, as of yet, no real work around UX design of the interface has occurred, and we mainly adhere to Mirador’s native UX choices. Mirador also supports themes which can radically change the feel of the environment (accessed through the left-most toolbar, under _Workspace settings > Change theme_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "### ML-driven workflow integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "The functionalities presented in the previous subsection allow for manual creation and modification of projects and annotations, principally allowing for work in a close-reading approach. We also wish, however, to integrate distant-reading and viewing techniques, allowing Arvest to be a complete environment to create content in these two complementary approaches and navigate between their output. Future development will seek to integrate curated ML algorithms directly within the tool, however in this first phase of development, we decided to propose an open API for the creation, consultation and modification of content within Arvest (be it media, projects or annotations) from afar. This allows us to integrate Arvest in complex ML-driven workflows without any extra overhead on the development end. As mentioned above, the multi-user environment is written in Django and designed to function with HTTP requests, allowing for the creation of a RESTful API. Furthermore, IIIF Manifests are interoperable JSON documents which can be created by any number of methods and be directly exploitable within Arvest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "With Arvest we are seeking to bridge the gap between non techno-fluent users and the kidneys of tools and environments discussed in the first part of this article. In the spirit of this project, we are currently assembling a [library of python notebooks](https://github.com/arvest-data-in-context/ml-notebooks) that allow for the creation of content that can be directly opened in Arvest using various ML algorithms and workflows. The notebooks are designed to be run with no installation needed directly online within [Google Colaboratory](https://colab.research.google.com/), and also with very little setup needed locally in [Anaconda](https://www.anaconda.com/). These notebooks are written with a pedagogical tone, and are designed to be novice-friendly, and robust enough to use in a workshop setting with the user's own content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "The notebooks are split into two different categories: first basic Arvest functionalities such as accessing and uploading media, reading and creating Manifests. We also offer utility scripts such as batch media uploads. The second category are common ML workflows that have been developed in the context of various case studies that are driving Arvest’s development. We discuss one such project in detail in the following section, and other case studies will be the subject of detailed analysis for future publications. However can already propose the following workflows:\n",
    "\n",
    "- Automatic speech recognition using [vosk](https://alphacephei.com/vosk/). Derived from a case study in theatrical studies, video or audio can be decomposed into words, and annotations are created for each word detected.\n",
    "- Image embedding projection with the [Distant Viewing toolkit](https://github.com/distant-viewing/dvt). Derived from a case study in history of art, a workflow similar to a tool like [PixPlot](https://dhlab.yale.edu/projects/pixplot/). The user can take a corpus of still images and use an image embedding neural network model and dimensionality reduction algorithms such as UMAP or T-SNE to project them into a space where images that are close to each other are similar. An interactive Manifest is created of the projection, where clicking on the image in the projection will open the original image in Arvest.\n",
    "- Video decomposition using the Distant Viewing toolkit. Derived from a case study in history, a large corpus of archival video footage we decomposed into shots and then projected into manifold spaces like described above in order to make navigation of the corpus easier.\n",
    "- Actor-network analysis using [networkx](https://networkx.org/) and link to other platforms. In the context of a case study around the archives of a music creation center, we perform analysis on the contents of the archives that are stored on [Nakala](https://www.nakala.fr/) and [Heurist](https://heuristnetwork.org/). We can create network visualizations of the various records that can be navigated in Arvest, and when a node is linked to a media file, the file can be viewed directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "One remarks that each of these case studies look at incorporating other digital humanities projects like vosk, the Distant Viewing toolkit, and platforms like Nakala and Heurist. Indeed, we intend that Arvest integrate itself into this digital humanities environment, and serve as a platform where various approaches and tools can work together seamlessly. This library of notebooks is also intended to give a map and a starting point for the various ML-driven workflows that are commonly used in digital humanities - it can not only be a resource to learn about feature extraction, dimensionality reduction, classification etc., but also a way of rapidly testing a workflow on a given corpus with immediate interactive output in a platform like Arvest, and a starting point in the code which can be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Arvest to interpret digital traces: deriving collaboration analytics from multimodal document networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following our co-design approach, we developed the prototype of Arvest and detailed its specifications along with different case studies. Wishing to anchor development of the tool in real scholarly practice, we have investigated the affordances and limitations of this kind of environment when put to the service of a semantic line of questioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present below a detailed case study centered around the Pilot 2 project Dancing Philosophy in the context of the EU Horizon 2020-funded COESO project (Collaborative Engagement on Societal Issues). This Pilot, led by dancer-choreographer Cosetta Graffione and philosopher Stefania Ferrando, looked to explore “_new ways of knowing through words and dance_” <cite data-cite=\"6386835/VNXFSQUM\"></cite>. Ferrando and Graffione met on seven different occasions from October 2021 to June 2022 in France and Italy where they lead workshops with dance students, other artists, researchers and local citizens <cite data-cite=\"6386835/7DMDF9LP\"></cite> (see also the blog documenting the project and its different steps: [https://coeso.hypotheses.org/498](https://coeso.hypotheses.org/498)). We collected the digital traces of this research creation, during its course and afterwards. The collection of traces consists of texts, photographs, videos, notebooks, and drawings (which have been digitized). How can we derive collaboration analytics from a collection of multimodal digital traces, and gain insight on the nature of the collaboration that occurred? How can automated analysis and the programmatic generation of data-driven navigation interfaces participate in a research project that looks to track an intangible, collaborative creative process? What can software offer us in terms of possibilities for interface and navigation of this information that will prolong our analysis and open new avenues for research?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing on Pilot 2, we looked to discover how collaboration allowed participants to displace/develop/question each other’s respective fields and practices (in this case, contemporary dance and philosophy), and to understand the nature of the collaborative processes that occurred. In order to best approach this question, we based our research on recent work from Boullier and Pidoux <cite data-cite=\"6386835/BGV8ZHSF\"></cite> that was also carried out in the context of the COESO project. In their paper, Boullier and Pidoux offer a conceptual framework for understanding the nature of a collaborative process. Methodologically, Boullier and Pidoux allow for the ongoing collection of data during a project. They intend that their methods and the artifacts they produce will participate in a feedback loop with a project’s participants, allowing them to reflect on (and recalibrate) the nature of the collaborative work they are undertaking. Working with a small corpus of traces that documents Graffione and Ferrando’s Pilot study, we wish to examine how Boullier and Pidoux’s model can be applied once an on-going project has been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their _collaborative analytics_, Boullier and Pidoux propose a number of cooperation features such as _skills_, _culture diversity_, _legal and ethical compliance_ etc. Each of these features are given categorical values that impact the _cooperation typology_ of a collaboration. The cooperation typology is visualized according to a compass model. A feature will be categorized into the _adaptive_, _plan oriented_, _institutional_ or _revisable_ poles of this compass. For each feature, different categories will correspond to different typological poles: for example, with _skills_, _experimental_ corresponds to _adaptive_, _academic-expert_ to _plan oriented_ and _procedural_ to _institutional_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boullier and Pidoux then construct a number of _indicators_ for each of these cooperation features. Some features have one indicator attached to them (for example, _skills_ has the indicator _skill type_), some several (for example _rhythm of citizen/research participation_ has _frequency of conversation and contributions_ and _frequency of meeting_). Each indicator has a number of different _data_ that inform it: for example, the _skill type_ indicator is derived from the data “_words to identify for every user and classify according to feature categories_”. These data are further attached to _data sources_ (for the previous example, the source is _message content_). Methods of analysis are proposed for each indicator, such as NLP, qualitative analysis, multidimensional analysis etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COESO Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our corpus is comprised of 34 documents, including videos, audios, photos, Laban notations, and written documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jdh": {
     "module": "object",
     "object": {
      "source": [
       "table 1: label table 1"
      ]
     }
    },
    "tags": [
     "data-table",
     "table-2"
    ]
   },
   "source": [
    "|#|Title|Type|Description|Date|Place|\n",
    "---|---|---|---|---|---\n",
    "1|Desire|Video|Edited recording of final public performance after workshop.|2021-10-03|St. Exupéry Cultural Centre of Wissous (France).\n",
    "2|Public workshop|Video|Edited recording of the final performance after workshop with members of the public.|2021-10-03|St. Exupéry Cultural Centre of Wissous (France).\n",
    "3|Public performance|Video|Edited recording of final public performance after workshop.|2022-04-03|St. Exupéry Cultural Centre of Wissous (France).\n",
    "4|Choreographic phrase recording|Video|Recording of CG working on a choreographic phrase.|2022-04-03|St. Exupéry Cultural Centre of Wissous (France).\n",
    "5|Burning heart|Video|Edited recordings of work during the workshop.|2022-04-11 – 2022-04-12|Hauts-de-France Polytechnic University, Arenberg (France).\n",
    "6|Wandering|Video|Edited recordings of work during the workshop.|2022-04-11 – 2022-04-12|Hauts-de-France Polytechnic University, Arenberg (France).\n",
    "7|Improvised solos|Video|Recording of CG and SF working on solos during workshop.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "8|Teaser of Ferrando’s solo|Video|Extraits of the _Impovised solos_ video teasing SF’s solo.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "9|Teaser of Graffione’s solo|Video|Extraits of the _Impovised solos_ video teasing CG’s solo.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "10|Alma Ballet Exercises|Video|Edited recording of different groups of students working on choreographies.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "11|Centre (Graffione’s description)|Audio|CG describes the movement around the word _Centre_.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "12|Centre (Ferrando’s description)|Audio|SF describes the movement around the word _Centre_.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "13|Organic (Graffione’s description)|Audio|CG describes the movement around the word _Organic_.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "14|Organic (Ferrando’s description)|Audio|SF describes the movement around the word _Organic_.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "15|Sober (Graffione’s description)|Audio|CG describes the movement around the word _Sober_.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "16|Sober (Ferrando’s description)|Audio|SF describes the movement around the word _Sober_.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "17|Truth (Graffione’s description)|Audio|CG describes the movement around the word _Truth_.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "18|Truth (Ferrando’s description)|Audio|SF describes the movement around the word _Truth_.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "19|Alma Ballet woskshop photos|Photos|Photos of the workshop that features CG, SF and the Alma Ballet students.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy)\n",
    "20|Laban Kinetography of a choreographic phrase|Laban Kinetography (PDF)|Laban Kinetography of the phrase worked on in the _Choreographic phrase recording_ video.|2022-04-03|St. Exupéry Cultural Centre of Wissous (France).\n",
    "21|Laban Kinetography|Laban Kinetography (PDF)|Laban Kinetography of the movements around the words described in the various audio recordings.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy).\n",
    "22|Alma Ballet Laban Kinetography|Laban Kinetography (PDF)|Laban Kinetography of the exercises performed by the Alma Ballet students.|2022-04-19 – 2022-04-23|Alma Danza Centre, Bologna (Italy).\n",
    "23|Freedom and the practice of authenticity in Italian feminism.|Written document (PDF)|Text written by SF for a presentation at a conference.|2021-11-03|Univeritat de Barcelona, Barcelona (Spain).\n",
    "24|“Starting with words” intentions|Written document (PDF)|Text stating the intentions for the workshop in Wissous.|2022-03-23 – 2022-04-02|St. Exupéry Cultural Centre of Wissous (France).\n",
    "25|About silence|Written document (PDF)|Text written by SF about _Silence_ (the word at the base of her solo).||Paris (France).\n",
    "26|Graffione’s intentions|Written document (PDF)|Text written by CG about the solos and the words that triggered them.|2022-04-01 – 2022-05-01|Paris (France).\n",
    "27|About the workshops|Written document (PDF)|A text recounting CG’s thoughts on the various workshops.|2022-05|Paris (France).\n",
    "28|Graffione’s sketches|Written document (PDF)|CG work documentation: photo of pages from a book in Italian.|2021-09|St. Exupéry Cultural Centre of Wissous (France).\n",
    "29|Founding texts|Written document (PDF)|Philosophical texts by Pina Bausch and Gualtieri.|2022-03-22 – 2022-04-03|St. Exupéry Cultural Centre of Wissous (France).\n",
    "30|Graffione Intentions|Written document (PDF)|CG writes about intentions for the workshops.|2022-05-01|Paris (France).\n",
    "31|Ferrando’s notes|Written document (PDF)|Photos of pages from SF’s notebooks.||St. Exupéry Cultural Centre of Wissous (France).\n",
    "32|Graffione’s notes|Written document (PDF)|Photos of pages from CG’s notebooks.|2022-03-01|St. Exupéry Cultural Centre of Wissous (France).\n",
    "33|Desire and relationships|Written document (PDF)|A text written by SF on desire and relationships.||EHESS-Paris and UPHF-Valenciennes (France), Savona (Italy).\n",
    "34|La parola del corpo|Written document (PDF)|A text written by CG and SF about the project.|2021-11-01 – 2022-03-01|EHESS-Paris (France), Savona (Italy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus is a curated sub-selection of the entire COESO and Pilot 2 archives: although small, this collection of multimodal documents still offers many perspectives on some of the outcomes of the Pilot study and the creative process that occurred. Some characteristics of this collection are worth noting:\n",
    "\n",
    "- They are _fragmentary_ documents. Somewhat contrary to many of the kinds of documents described in Boullier and Pidoux’s model, these documents emerge in a sporadic manner. We cannot hope to attain a complete overview of the project in an ongoing, continuous manner; only certain moments in time that find themselves entwined within these artifacts.\n",
    "- They are _subject to mediation_. Contrary to the kind of raw data that Boullier and Pidoux manipulate, these documents have been edited, curated and selected by various actors.\n",
    "- They are _aesthetic objects_. Many of these objects emerge from a creative process. Some document events, some are final reports or recordings of performances, some are between these poles. In any case, they differ from the communicative, day-to-day human interaction sources proposed by Boullier and Pidoux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite these differences, is it still possible to attain collaborative metrics like those proposed by Boullier and Pidoux from this content? Can we measure collaboration from this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "### Cooperation features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "Given the nature of collaborative work and the artifacts that emerge from it, we found it useful to conceive of a document as a node in a network. A first task was to examine the various indicators proposed by Boullier and Pidoux and see to what extent they can be adapted and measured when the data source is one of these nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "[Here](https://github.com/arvest-data-in-context/COESO-collaborative-analytics/blob/main/Data/boullier-pidoux-assessment.pdf) the reader may find a complete assessment of the cooperation features, categorical values, indicators and corresponding data sources proposed by Boullier and Pidoux and how we propose to adapt them to our corpora. The reader will notice an immediate tension between some of the metrics proposed and how these are to be categorized into typological categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "Let us take an example with the feature _Dissemination type of results_, the indicator for which is _degree of field hybridation_. Boullier and Pidoux originally propose to derive this from _NLP analysis of reports_, _a list of actors that disseminate the product_, _where a product is disseminated_, _what is the type of product being disseminated_ and _what the format of the product is_. These can be qualified as quantitative metrics, however there is no indication as to how they then inform the three categorical values of _open_ (_adaptive_), _divided according to fields_ (_plan-oriented_) and _academic oriented_ (_institutional_). Indeed, it appears to be left up to the researcher to assess these fields and then categorize the results in a qualitative manner. In our assessment, we indicate how these categorizations could be considered for documents in the column _Node data + method of analysis_. Considering that even with quantitative gathering of certain metrics the final qualitative categorization will ultimately be left to the researcher, and given the small size of the corpus, the decision was made here to bypass the step of quantitative data collection and allow the researcher to make a categorization for each feature straight away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "The categorization was performed for each node in our document collection. Next, this data was taken and ‘normalized’ so that each document could be placed on Boullier and Pidoux’s typological compass. In Figure 1 the reader can find an example of two different documents visualized in this manner (on the left, the video _Desire_ and on the right _Ferrando’s notes_). We also calculate the centroid of this resulting polygon so that it may be represented by one point on the compass. This allows for a composite visualization of the entire collection which can be viewed in Figure 2. The reader can also consult the [full results](https://github.com/arvest-data-in-context/COESO-Collaborative-Analytics/blob/main/Appendix/2-Composite-Typology-Visualization-Data.json) here for the full composite data file used to construct the final visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hermeneutics",
     "figure-typological-compass-placement-1"
    ]
   },
   "outputs": [],
   "source": [
    "metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Placement on the typological compass of Desire (left) and Ferrando's notes (right).\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "display(Image(\"media/figure1.png\", width=1000), metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hermeneutics",
     "figure-typological-compas-composite-2"
    ]
   },
   "outputs": [],
   "source": [
    "metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Composite view of the centroid of all document placements in the collection on the typological compass.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "display(Image(\"media/figure2.png\", width=1000), metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hermeneutics"
    ]
   },
   "source": [
    "The normalizations were obtained by taking the typology matrix for each feature and getting an average for each typology. Note that we do not include the features that were not informed for the document. The absence of certain fields from some documents can weigh the visualization somewhat – we have not found a solution for accounting for this in the visualization itself, however, in the [full results](https://github.com/arvest-data-in-context/COESO-Collaborative-Analytics/blob/main/Appendix/2-Composite-Typology-Visualization-Data.json) the reader can also find the loss field for each document, which is a metric indicating the percentage of fields that were informed (a loss of 0.2 means that 80% of features were informed). This is a factor that is to be considered when interpreting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration analytics with Arvest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these vizualisations, different questions arise. For example, why are most of the documents oriented towards the _plan-oriented_ typology? What are the three points in the _adaptive_ category? What are the two outlying points? Before we begin to answer these questions, it is clear that a dynamic way of navigating this multidimensional data would be useful (the various dimensions are: feature value for each document; composite for each document; composite for the entire collection). This is where our tool Arvest can offer some solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levying Python scripting we can rapidly create interactive visualizations for the Arvest environment. In Figure 3, we see the data we have created loaded as a [manifest network](https://coeso.tetras-libre.fr/data/coeso-deliverable/c41b0c91-f735-41fa-83f0-d9f836bb9ca1.json) in [Arvest](https://coeso.tetras-libre.fr/). On the left, in the composite visualization of the document collection, each point on the compass is linked to an annotation which allows the reader to open the corresponding document’s Manifest in another pane in Arvest. On the right, one of these Manifests features the document in question, and also an annotation that paints the document’s typological compass below it. This allows us to rapidly navigate this multidimensional data, pivoting between different views, and even simultaneously viewing the distant and close reading perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "figure-manifest-network-navigation-3"
    ]
   },
   "outputs": [],
   "source": [
    "metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Manifest network derived from the collaborative typology data loaded into Arvest.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "display(Image(\"media/figure3.png\", width=1000), metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection tends towards the _plan-oriented_ corner of the compass. This is defined by Boullier and Pidoux as consisting of collaborations of short-duration and of a relatively formalized nature. The pilot gave place to a series of short workshops which were led according to a formalized scientific method. This composite view and typological tendency confirm the suppositions that we could make about the collaborative work that occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a closer look, we have identified some notable points of interest which are labeled in Figure 4. We shall explore these documents using Arvest and offer some interpretation as to how they may have ended up in the places they occupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "figure-clusters-4"
    ]
   },
   "outputs": [],
   "source": [
    "metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Decomposition of the composite view into clusters.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "display(Image(\"media/figure4.png\", width=1000), metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents in Grouping A are outliers compared to the rest of the collection of documents. They are in fact 8 documents which are all identical in typological nature, and therefore superimposed on the visualization. The 8 documents are the audio recordings made by Graffione and Ferrando that describe the movements depicted in the Laban notations around the four words _truth_, _organic_, _sober_ and _center_. They all tend towards the _institutional_ corner of the compass. When looking through the data, we see that this can be explained by the fact that the recordings were made specifically for a final report for the COESO Project <cite data-cite=\"6386835/7DMDF9LP\"></cite>. They are only accessible through a link that is made available through this scientific paper, and it can be argued that had they not have had to write this report, these recordings (which ‘make up’ for an absence of video documentation) would not have been made. Furthermore, they each involve one actor, speaking in their own language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents in Grouping B are comprised of videos documenting work that occurred during public workshops. They are the only documents in the collection that breach the line from _plan-oriented_ towards the _adaptive_ corner of the compass. The contextual network of the documents is growing through reaching out to the public. They do not stray too far from the pack – indeed, as open and adaptive as these workshops need to be, they are still run with a scientific and formalized methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metadata={\n",
    "    \"jdh\": {\n",
    "        \"module\": \"object\",\n",
    "        \"object\": {\n",
    "            \"type\":\"image\",\n",
    "            \"source\": [\n",
    "                \"Local compass placement of Ferrando's notebooks.\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "display(Image(\"media/figure5.png\", width=1000), metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping C describes two documents: photos of _Graffione_ and _Ferrando’s notebooks_. Their positions on the compass throw light on an eventual limit to the methods we are exploring. When looking at the local typological compass (see Figure 5), we see that the documents in fact tend extremely towards the adaptive and institutional corners of the compass, hence the final position which is derived as the centroid of all informed fields. Note also that these documents have an extreme _loss_ factor: 0.46, meaning that barely half of the fields could actually be informed. This pushes on the importance of having access to different perspectives when looking at this kind of multidimensional, data-driven interface as quantitative analysis can sometimes hide the realities of the dataset it is describing. We can perhaps explain this typology as being derived from a low amount of plan-oriented work (these are personal notebooks in which the actor writes in an ad hoc manner)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping D is comprised of textual documents such as _Graffione’s intentions_ and _Laban Notations_. These documents tend the most towards the _plan-oriented_ corner of the compass. Indeed, they either derive from or state a somewhat rigid methodology that steers the document away from the adaptive side of the scale. The _Laban notations_ notably also require an expert knowledge to be interpreted, and therefore tend the document away from _adaptability_ and _openness_. These are also documents created by a sole author. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Grouping E is comprised of an ad hoc collection of documents and can be considered to represent the general tendency of the collection’s typology. We notably find textual documents that offer overviews of the Pilot’s workings and methods such as _Choreographic score_, _About the workshops_, _About silence_, _La parola del corpo_ and _Desire and relationships_ as well as the two video documents that are made as teasers for the project. Their central position can therefore be accounted for by their global perspective on the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer to the challenge of interpreting digital traces in a digital environment, combining qualitative and quantitative methods, traditional and innovative approaches, we propose Arvest, a solution dedicated first to historians but which can be of interest to other fields. This paper presents the history and the different development phases which structured the development, as well the main issues Arvest has faced and its specifications. The development, based on a co-design approach, is ongoing, along with case studies. We presented a case study based on the COESO project. We adopted Boullier and Pidoux’s model of collaborative analytics, and examined if it was possible to apply this model to a collection of multimodal documents. The possibilities offered by Arvest allowed us to navigate our multidimensional, data-driven interface and interrogate the documents in new ways. The multiple layers of analysis engendered an analytical process that encouraged new ways of perceiving the corpus. Essentially, the visualizations were a means of rendering and engaging with the researchers’ own subjective inflections and suppositions around a corpus in a structured manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prototype of Arvest is already available and a beta version will be released by the end of 2024. It is part of the ongoing development of the ERC-funded project From Stage to Data, the Digital Turn of Contemporary Performing Arts Historiography (STAGE). Arvest will be used to model creative processes and particularly their collaborative dimension, taking into account not only the rehearsals but also all the data produced by the team members. The major point in terms of development going forward for Arvest is a need to dynamically create bespoke interfaces like the network and composite typological interfaces shown in the COESO case study. These interfaces are a powerful tool to access many different data-driven perspectives on the collection. As demonstrated with the typological compass, the nature of these interfaces will differ according to the type of analysis and research question being carried out. It will be of utmost importance to offer tools for the programmatic and manual creation of these interfaces within the tool going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funded by the European Union (ERC, STAGE, grant agreement no. 101097091). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le soutien de la Région Bretagne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le soutien de Ouest Valorisation, Société d’Accélération du Transfert de Technologies."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "citation-manager": {
   "items": {}
  },
  "cite2c": {
   "citations": {
    "6386835/348ACRBE": {
     "URL": "https://distant-viewing.github.io/dvt/index.html",
     "accessed": {
      "day": 20,
      "month": 12,
      "year": 2019
     },
     "author": [
      {
       "family": "Arnold",
       "given": "Taylor"
      },
      {
       "family": "Tilton",
       "given": "Lauren"
      }
     ],
     "id": "6386835/348ACRBE",
     "issued": {
      "year": 2019
     },
     "title": "The Distant Viewing Toolkit",
     "type": "book",
     "version": "0.3.1"
    },
    "6386835/3AYTRGVP": {
     "abstract": "In this paper we discuss the intricacies of innovation leadership behaviour and design thinking as drivers and enablers of organizational innovation. We propose transformational leadership (TFL) and design thinking (DT) capabilities as suitable for alleviating issues of business innovation. Managers and the processes they apply, the behaviours they exert and the work cultures they promote are shaping the organisational practices and culture in which innovation occurs. As we explore the DT process and mindset and further conceptualise and interpreted it in light of transformational leadership theory we find that transformational leadership offers a theoretical lens through which the potential performance enhancing effects of DT can be explained. A conceptual model of design innovation leadership is proposed.",
     "author": [
      {
       "family": "Groeger",
       "given": "Lars"
      },
      {
       "family": "Schweitzer",
       "given": "Jochen"
      }
     ],
     "id": "6386835/3AYTRGVP",
     "issued": {
      "day": 3,
      "month": 7,
      "year": 2014
     },
     "title": "Transformational leadership, design thinking and the innovative organization",
     "type": "paper-conference"
    },
    "6386835/3ILX8VAL": {
     "ISBN": "978-90-8964-291-2",
     "URL": "https://www.oapen.org/record/530353",
     "accessed": {
      "day": 6,
      "month": 7,
      "year": 2018
     },
     "editor": [
      {
       "family": "Noordegraaf",
       "given": "Julia"
      },
      {
       "family": "Saba",
       "given": "Cosetta G."
      },
      {
       "family": "Le Maître",
       "given": "Barbara"
      },
      {
       "family": "Hediger",
       "given": "Vinzenz"
      }
     ],
     "event-place": "Amsterdam",
     "id": "6386835/3ILX8VAL",
     "issued": {
      "year": 2013
     },
     "language": "en",
     "note": "DOI: 10.26530/OAPEN_530353",
     "publisher": "Amsterdam University Press",
     "publisher-place": "Amsterdam",
     "shortTitle": "Preserving and Exhibiting Media Art",
     "title": "Preserving and Exhibiting Media Art: Challenges and Perspectives",
     "title-short": "Preserving and Exhibiting Media Art",
     "type": "book"
    },
    "6386835/3QQEILFY": {
     "DOI": "10.1093/llc/fqz013",
     "URL": "https://doi.org/10.1093/llc/fqz013",
     "abstract": "In this article we establish a methodological and theoretical framework for the study of large collections of visual materials. Our framework, distant viewing, is distinguished from other approaches by making explicit the interpretive nature of extracting semantic metadata from images. In other words, one must ‘view’ visual materials before studying them. We illustrate the need for the interpretive process of viewing by simultaneously drawing on theories of visual semiotics, photography, and computer vision. Two illustrative applications of the distant viewing framework to our own research are draw upon to explicate the potential and breadth of the approach. A study of television series shows how facial detection is used to compare the role of actors within the narrative arcs across two competing series. An analysis of the Farm Security Administration–Office of War Information corpus of documentary photography is used to establish how photographic style compared and differed amongst those photographers involved with the collection. We then aim to show how our framework engages with current methodological and theoretical conversations occurring within the digital humanities.",
     "accessed": {
      "day": 3,
      "month": 10,
      "year": 2021
     },
     "author": [
      {
       "family": "Arnold",
       "given": "Taylor"
      },
      {
       "family": "Tilton",
       "given": "Lauren"
      }
     ],
     "container-title": "Digital Scholarship in the Humanities",
     "container-title-short": "Digital Scholarship in the Humanities",
     "id": "6386835/3QQEILFY",
     "issue": "Supplement_1",
     "issued": {
      "day": 1,
      "month": 12,
      "year": 2019
     },
     "journalAbbreviation": "Digital Scholarship in the Humanities",
     "page": "i3-i16",
     "page-first": "i3",
     "shortTitle": "Distant viewing",
     "title": "Distant viewing: analyzing large visual corpora",
     "title-short": "Distant viewing",
     "type": "article-journal",
     "volume": "34"
    },
    "6386835/6XP93NMB": {
     "DOI": "10.5281/zenodo.6834643",
     "URL": "https://zenodo.org/records/6834643",
     "abstract": "The Fluid Corpus Manipulation Toolkit enables techno-fluent artists to tackle programmatic data mining within their mastered creative coding environment, by integrating advances of signal decomposition, machine listening and machine learning to the toolset of musicians.  It is made of software extensions for Max, SuperCollider and Pure Data, as well as a learning platform and a forum to exchange on the emerging practices.",
     "accessed": {
      "day": 13,
      "month": 3,
      "year": 2024
     },
     "author": [
      {
       "family": "Tremblay",
       "given": "Pierre Alexandre"
      },
      {
       "family": "Green",
       "given": "Owen"
      },
      {
       "family": "Roma",
       "given": "Gerard"
      },
      {
       "family": "Bradbury",
       "given": "James"
      },
      {
       "family": "Moore",
       "given": "Ted"
      },
      {
       "family": "Hart",
       "given": "Jacob"
      },
      {
       "family": "Harker",
       "given": "Alex"
      }
     ],
     "id": "6386835/6XP93NMB",
     "issued": {
      "day": 7,
      "month": 7,
      "year": 2022
     },
     "language": "eng",
     "note": "Publisher: Zenodo",
     "title": "Fluid Corpus Manipulation Toolbox",
     "type": "article-journal"
    },
    "6386835/77JUQC9J": {
     "ISBN": "978-1-4724-4324-3",
     "abstract": "\"Data and its technologies now play a large and growing role in humanities research and teaching. This book addresses the needs of humanities scholars who seek deeper expertise in the area of data modeling and representation. The authors, all experts in digital humanities, offer a clear explanation of key technical principles, a grounded discussion of case studies, and an exploration of important theoretical concerns. The book opens with an orientation, giving the reader a history of data modeling in the humanities and a grounding in the technical concepts necessary to understand and engage with the second part of the book. The second part of the book is a wide-ranging exploration of topics central for a deeper understanding of data modeling in digital humanities. Chapters cover data modeling standards and the role they play in shaping digital humanities practice, traditional forms of modeling in the humanities and how they have been transformed by digital approaches, ontologies which seek to anchor meaning in digital humanities resources, and how data models inhabit the other analytical tools used in digital humanities research. It concludes with a glossary chapter that explains specific terms and concepts for data modeling in the digital humanities context. This book is a unique and invaluable resource for teaching and practising data modeling in a digital humanities context\"--",
     "author": [
      {
       "family": "McCarty",
       "given": "Willard"
      }
     ],
     "call-number": "AZ105 .S43 2019",
     "collection-title": "Digital research in the arts and humanities",
     "container-title": "The shape of data in the digital humanities: modeling texts and text-based resources",
     "editor": [
      {
       "family": "Flanders",
       "given": "Julia"
      },
      {
       "family": "Jannidis",
       "given": "Fotis"
      }
     ],
     "event-place": "London ; New York",
     "id": "6386835/77JUQC9J",
     "issued": {
      "year": 2019
     },
     "language": "en",
     "page": "264-284",
     "page-first": "264",
     "publisher": "Routledge, Taylor & Francis Group",
     "publisher-place": "London ; New York",
     "title": "Modeling the actual, simulating the possible",
     "type": "chapter"
    },
    "6386835/7DMDF9LP": {
     "DOI": "10.5281/zenodo.6788008",
     "URL": "https://zenodo.org/record/6788008",
     "abstract": "This document is one of the documentations of Dancing Philosophy, that is the central work of the Pilot 2 of the European project on citizen sciences COESO. It documents the collaborative research work developed by Cosetta Graffione, dancer and choreographer, and Stefania Ferrando, philosopher. Based on philosophical questions, Dancing Philosophy experiments “new ways of knowing through speech and dance” (quote from the COESO D2.6 report of Stefania Ferrando: “The movement of an embodied thought”, June 29th 2022).",
     "accessed": {
      "day": 1,
      "month": 11,
      "year": 2022
     },
     "author": [
      {
       "family": "Blin",
       "given": "Irénée"
      },
      {
       "family": "Marranca",
       "given": "Daniele"
      }
     ],
     "id": "6386835/7DMDF9LP",
     "issued": {
      "day": 29,
      "month": 6,
      "year": 2022
     },
     "language": "eng",
     "note": "Publisher: Zenodo",
     "title": "Dancing Philosophy - Choreographic Score (COESO D.2.5)",
     "type": "article-journal"
    },
    "6386835/7JUQ27DG": {
     "ISBN": "978-1-78326-608-1 978-1-78326-637-1",
     "author": [
      {
       "family": "Graham",
       "given": "Shawn"
      },
      {
       "family": "Milligan",
       "given": "Ian"
      },
      {
       "family": "Weingart",
       "given": "Scott"
      }
     ],
     "call-number": "D16.117 .G73 2016",
     "event-place": "London",
     "id": "6386835/7JUQ27DG",
     "issued": {
      "year": 2016
     },
     "language": "en",
     "number-of-pages": "282",
     "publisher": "Imperial College Press",
     "publisher-place": "London",
     "shortTitle": "Exploring big historical data",
     "title": "Exploring big historical data: the historian's macroscope",
     "title-short": "Exploring big historical data",
     "type": "book"
    },
    "6386835/7R9S2WPF": {
     "URL": "http://www.digitalhumanities.org/dhq/vol/14/3/000476/000476.html",
     "author": [
      {
       "family": "Bardiot",
       "given": "Clarisse"
      }
     ],
     "container-title": "Digital Humanities Quarterly",
     "container-title-short": "DHQ",
     "id": "6386835/7R9S2WPF",
     "issue": "3",
     "issued": {
      "year": 2020
     },
     "journalAbbreviation": "DHQ",
     "shortTitle": "Theatre analytics",
     "title": "Theatre analytics: developing software for theatre research",
     "title-short": "Theatre analytics",
     "type": "article-journal",
     "volume": "14"
    },
    "6386835/8N72JUGA": {
     "DOI": "10.1080/14794713.2021.1884803",
     "URL": "https://doi.org/10.1080/14794713.2021.1884803",
     "abstract": "Choreographer William Forsythe calls the work Duo a ‘project’—reflecting the piece’s long history of vicissitudes from 1996 to the present. We attempt to visualize continuity and change over several iterations of Duo, spanning a period of 20 years. Our methods involved graphical and statistical approaches to performance video annotation, considering seven videos acquired from Forsythe’s private archive. Collaboration with Duo dancers was critical to develop this choreographic knowledge. The duet Duo was chosen to focus on annotation of partnering, choreographic structure, and interpretation; the case study furthermore enabled review of annotation methods from Forsythe’s Synchronous Objects for One Flat Thing, reproduced (Forsythe et al. 2009) and built upon prior research of entrainment in Duo (Waterhouse, Watts, and Bläsing 2014). Studying a choreography longitudinally, with close regard of the performers’ testimonies and digital traces, the problem required innovative methods. For this article, we focus on how annotation was used within this project. We outline our particular interdisciplinary approach, merging perspectives from dance studies, praxeology and creative coding. We present the language and concepts of annotation chosen, technical tools used for annotating, procedures of annotation analysis, and conclusions of the research. Thereby we present novel visualizations of choreographic process.",
     "accessed": {
      "day": 27,
      "month": 9,
      "year": 2022
     },
     "author": [
      {
       "family": "Waterhouse",
       "given": "Elizabeth"
      },
      {
       "family": "Jenett",
       "given": "Florian"
      },
      {
       "family": "Hager",
       "given": "Monika"
      },
      {
       "family": "Coniglio",
       "given": "Mark"
      }
     ],
     "container-title": "International Journal of Performance Arts and Digital Media",
     "id": "6386835/8N72JUGA",
     "issue": "1",
     "issued": {
      "day": 2,
      "month": 1,
      "year": 2021
     },
     "note": "Publisher: Routledge\n_eprint: https://doi.org/10.1080/14794713.2021.1884803",
     "page": "160-181",
     "page-first": "160",
     "title": "‘I gave that cue.’ Integrating dance studies, praxeology, and computational perspectives to model change in the case study of William Forsythe’s <i>Duo</i>",
     "type": "article-journal",
     "volume": "17"
    },
    "6386835/9BB8NEPZ": {
     "ISBN": "978-0-321-93407-9",
     "author": [
      {
       "family": "Cairo",
       "given": "Alberto"
      }
     ],
     "call-number": "QC397.5.I53 C35 2016",
     "event-place": "Place of publication not identified",
     "id": "6386835/9BB8NEPZ",
     "issued": {
      "year": 2016
     },
     "note": "OCLC: ocn941982960",
     "number-of-pages": "382",
     "publisher": "New Riders",
     "publisher-place": "Place of publication not identified",
     "shortTitle": "The truthful art",
     "title": "The truthful art: data, charts, and maps for communication",
     "title-short": "The truthful art",
     "type": "book"
    },
    "6386835/BGV8ZHSF": {
     "DOI": "10.5281/zenodo.5599052",
     "URL": "https://zenodo.org/record/5599052",
     "abstract": "The Deliverable D5.1 WP5 - Cooperation Quality Assessment: Development and Implementation of the Cooperation Analytics is organized into five sections covering the conceptualisation and operationalisation of indicators to measure multiple cooperation practices. After framing the scope and purpose of WP5 within COESO in section II, the deliverable presents first, in section III, a state of the art on the cooperation definition, the features that define more specifically cooperation, along with the methods of measurement in the literature resulting in a conceptual grid. The conceptual grid is relevant as it justifies the construction of indicators we define in the following sections. Second, in section IV, the cooperation analytics’ monitoring grid is developed in detail. It represents the main conceptualisation stage of cooperation analytics. In this section we create a correspondence table between every feature retained and its level of analysis from the bibliography. In this section, we define every feature as it will be adopted in VERA’s cooperation analytics. Third, section V represents the core of the cooperation analytics operationalisation. However, it is important to note that the operationalisation is still at a conceptual design phase, as the VERA platform is not developed yet. But this step constitutes an important translation of the concepts into criteria that can be quantified and presented visually to practitioners and social scientists. In this section, the monitoring grid includes an operationalisation of cooperation based on four levels of granular analysis: ● a typology of cooperation, ● the cooperation features that define four types of cooperation, ● the categorical values for each cooperation feature in link with every cooperation type, ● the indicator construction, ● the data and text corpora to be collected within VERA along with their analysis methods. This section ends with the analysis of data visualization possibilities for the monitoring grid. Here we present the five main options retained. The indicators that will be presented to VERA users in the form of a monitoring grid are not a mere replication of the user activity tracked. Instead, the indicators will assess more broadly the cooperation practices in an aggregated way. The reason for this choice is that indicators, as a feedback, can directly affect the behavior of participants, to the point where they may try to conform to the scores and metrics, following what is called the “Goodheart effect” (a metrics that becomes the target of a behavior becomes a nasty metrics; i.e. used for autoreferential conformist purposes and not for reflexivity purposes anymore.) Fourth, in section VI, the deliverable provides an overview of the five Pilots’ state of progress based on interviews we conducted and additional ones conducted by Net7. Here we present five main observations we made on the Pilots’ state of progress from an exploratory analysis. The section includes our main contribution and the challenges we are facing to develop cooperation analytics. Finally, section VII concludes the document with the next steps for reviewing, implementing and testing the cooperation analytics here defined. The final section provides a view on where we are heading: to a close and experimental collaboration phase with Net7 and the Pilots that contributes to the development of cooperation analytics along with VERA’s core functionalities.",
     "accessed": {
      "day": 2,
      "month": 11,
      "year": 2021
     },
     "author": [
      {
       "family": "Boullier",
       "given": "Dominique"
      },
      {
       "family": "Pidoux",
       "given": "Jessica"
      }
     ],
     "id": "6386835/BGV8ZHSF",
     "issued": {
      "day": 30,
      "month": 9,
      "year": 2021
     },
     "language": "eng",
     "note": "Publisher: Zenodo",
     "shortTitle": "COESO Deliverable 5.1",
     "title": "COESO Deliverable 5.1: Cooperation Quality Assessment: Development and Implementation of the Cooperation Analytics",
     "title-short": "COESO Deliverable 5.1",
     "type": "article-journal"
    },
    "6386835/DEJEYRGC": {
     "URL": "http://chnm.gmu.edu/resources/essays/scarcity.php",
     "accessed": {
      "day": 21,
      "month": 1,
      "year": 2020
     },
     "author": [
      {
       "family": "Rosenzweig",
       "given": "Roy"
      }
     ],
     "container-title": "American Historical Review",
     "id": "6386835/DEJEYRGC",
     "issue": "3",
     "issued": {
      "month": 6,
      "year": 2003
     },
     "page": "735-762",
     "page-first": "735",
     "title": "Scarcity or Abundance? Preserving the Past in a Digital Era",
     "type": "article-journal",
     "volume": "108"
    },
    "6386835/E8VRJPYX": {
     "URL": "https://archivesic.ccsd.cnrs.fr/sic_00180185/",
     "accessed": {
      "day": 19,
      "month": 7,
      "year": 2017
     },
     "author": [
      {
       "family": "Zacklad",
       "given": "Manuel"
      }
     ],
     "container-author": [
      {
       "family": "Skare",
       "given": "Roswitha"
      },
      {
       "family": "Windfeld Lund",
       "given": "Niels"
      },
      {
       "family": "Varheim",
       "given": "Andreas"
      }
     ],
     "container-title": "A Document (Re)turn",
     "id": "6386835/E8VRJPYX",
     "issued": {
      "year": 2007
     },
     "page": "279-297",
     "page-first": "279",
     "publisher": "Peter Lang",
     "title": "Réseaux et communautés d'imaginaire documédiatisées",
     "type": "chapter"
    },
    "6386835/G36NI5EQ": {
     "DOI": "10.1109/WDM.2003.1233883",
     "URL": "https://ieeexplore.ieee.org/document/1233883",
     "abstract": "The promise of recent technological and legislative developments to facilitate the digital dissemination of music is undermined by the lack of reliable means to preserve accurate copies of digital files: music that can be easily transmitted and played back today may not be retrievable tomorrow. Preserving interactive music compositions is particularly problematic, as their performance typically relies on a variety of specialized components. We describe the planned research activities of MUSTICA, an international team of archivists, information scientists, and musicologists that seeks to develop tools to guide the preservation and presentation of interactive digital musical compositions in accordance with the standards and strategies for electronic records preservation being developed by MUSTICA's parent research initiative, InterPARES 2.",
     "accessed": {
      "day": 9,
      "month": 7,
      "year": 2024
     },
     "author": [
      {
       "family": "Bachimont",
       "given": "Bruno"
      },
      {
       "family": "Blanchette",
       "given": "Jean-François"
      },
      {
       "family": "Gerzso",
       "given": "Andrew"
      },
      {
       "family": "Swetland",
       "given": "Anne"
      },
      {
       "family": "Lescurieux",
       "given": "Olivier"
      },
      {
       "family": "Morizet-Mahoudeaux",
       "given": "Pierre"
      },
      {
       "family": "Donin",
       "given": "Nicolas"
      },
      {
       "family": "Teasley",
       "given": "Jill"
      }
     ],
     "container-title": "Proceedings Third International Conference on WEB Delivering of Music",
     "event": "Proceedings Third International Conference on WEB Delivering of Music",
     "id": "6386835/G36NI5EQ",
     "issued": {
      "month": 9,
      "year": 2003
     },
     "page": "109-112",
     "page-first": "109",
     "shortTitle": "Preserving interactive digital music",
     "title": "Preserving interactive digital music: a report on the MUSTICA research initiative",
     "title-short": "Preserving interactive digital music",
     "type": "paper-conference"
    },
    "6386835/HIGC4I2B": {
     "URL": "http://www.digitalhumanities.org/dhq/vol/14/3/000485/000485.html",
     "abstract": "Although the concept of digital comes with an assumption of placelessness and detachment from physical space and geographical location, these matters still play a significant role in the way the digital humanities research is practiced today, and also in the future. The location, the surroundings and infrastructure open the questions of accessibility and equality: space shapes the opportunities for doing digital humanities research, both enables and hinders collaboration, and both unifies and divides scholars.",
     "accessed": {
      "day": 30,
      "month": 5,
      "year": 2024
     },
     "author": [
      {
       "family": "Oiva",
       "given": "Mila"
      },
      {
       "family": "Pawlicka-Deger",
       "given": "Urszula"
      }
     ],
     "container-title": "Digital Humanities Quarterly",
     "id": "6386835/HIGC4I2B",
     "issue": "3",
     "issued": {
      "year": 2020
     },
     "title": "Lab and Slack. Situated Research Practices in Digital Humanities - Introduction to the DHQ Special Issue.",
     "type": "article-journal",
     "volume": "14"
    },
    "6386835/IN3DXMC2": {
     "ISBN": "978-1-00-902605-5 978-1-00-901252-2",
     "URL": "http://www.cambridge.org/core/elements/transformation-of-historical-research-in-the-digital-age/30DFBEAA3B753370946B7A98045CFEF4",
     "abstract": "Cambridge Core - Global History - The Transformation of Historical Research in the Digital Age",
     "accessed": {
      "day": 22,
      "month": 11,
      "year": 2022
     },
     "author": [
      {
       "family": "Milligan",
       "given": "Ian"
      }
     ],
     "id": "6386835/IN3DXMC2",
     "issued": {
      "month": 8,
      "year": 2022
     },
     "language": "en",
     "note": "Publisher: Cambridge University Press",
     "publisher": "Cambridge University Press",
     "title": "The Transformation of Historical Research in the Digital Age",
     "type": "book"
    },
    "6386835/J938J5MK": {
     "ISBN": "978-2-7578-4438-0",
     "author": [
      {
       "family": "Prost",
       "given": "Antoine"
      }
     ],
     "call-number": "907.2",
     "collection-number": "H225",
     "collection-title": "Points",
     "edition": "Édition augmentée",
     "event-place": "Paris",
     "id": "6386835/J938J5MK",
     "issued": {
      "year": 2014
     },
     "language": "fre",
     "publisher": "Points",
     "publisher-place": "Paris",
     "title": "Douze leçons sur l'histoire",
     "type": "book"
    },
    "6386835/JD3GQRLV": {
     "author": [
      {
       "family": "Manning",
       "given": "Patrick"
      }
     ],
     "event-place": "New York",
     "id": "6386835/JD3GQRLV",
     "issued": {
      "year": 2013
     },
     "publisher": "Palgrave Macmillan",
     "publisher-place": "New York",
     "title": "Big Data in History",
     "type": "book"
    },
    "6386835/JI5V3DFF": {
     "ISBN": "979-10-91281-93-5",
     "author": [
      {
       "family": "Rygiel",
       "given": "Philippe"
      }
     ],
     "collection-title": "Papiers",
     "event-place": "Villeurbanne",
     "id": "6386835/JI5V3DFF",
     "issued": {
      "year": 2017
     },
     "number-of-pages": "207",
     "publisher": "Presses de l'Enssib",
     "publisher-place": "Villeurbanne",
     "shortTitle": "Historien à l'âge du numérique",
     "title": "Historien à l'âge numérique",
     "title-short": "Historien à l'âge du numérique",
     "type": "book"
    },
    "6386835/JSS687WM": {
     "author": [
      {
       "family": "Drucker",
       "given": "Johanna"
      }
     ],
     "container-title": "Digital Humanities Quarterly",
     "container-title-short": "DHQ",
     "id": "6386835/JSS687WM",
     "issue": "1",
     "issued": {
      "day": 10,
      "month": 3,
      "year": 2011
     },
     "journalAbbreviation": "DHQ",
     "title": "Humanities Approaches to Graphical Display",
     "type": "article-journal",
     "volume": "005"
    },
    "6386835/L4J5MYTK": {
     "abstract": "More than twenty years after Clues, Myths, and the Historical Method was first published in English, this extraordinary collection remains a classic. The book brings together essays about Renaissance witchcraft, National Socialism, sixteenth-century Italian painting, Freud’s wolf-man, and other topics. In the influential centerpiece of the volume Carlo Ginzburg places historical knowledge in a long tradition of cognitive practices and shows how a research strategy based on reading clues and traces embedded in the historical record reveals otherwise hidden information. Acknowledging his debt to art history, psychoanalysis, comparative religion, and anthropology, Ginzburg challenges us to retrieve cultural and social dimensions beyond disciplinary boundaries.In his new preface, Ginzburg reflects on how easily we miss the context in which we read, write, and live. Only hindsight allows some understanding. He examines his own path in research during the 1970s and its relationship to the times, especially the political scenes of Italy and Germany. Was he influenced by the environment, he asks himself, and if so, how? Ginzburg uses his own experience to examine the elusive and constantly evolving nature of history and historical research.",
     "author": [
      {
       "family": "Ginzburg",
       "given": "Carlo"
      }
     ],
     "event-place": "Baltimore and London",
     "id": "6386835/L4J5MYTK",
     "issued": {
      "year": 1989
     },
     "language": "en",
     "number-of-pages": "241",
     "publisher": "John Hopkins University Press",
     "publisher-place": "Baltimore and London",
     "title": "Clues, Myths, and the Historical Method",
     "translator": [
      {
       "family": "Tedeschi",
       "given": "Anne C."
      },
      {
       "family": "Tedeschi",
       "given": "John"
      }
     ],
     "type": "book"
    },
    "6386835/M25WPB3U": {
     "DOI": "10.1145/3494837",
     "URL": "https://doi.org/10.1145/3494837",
     "abstract": "Intangible cultural heritage (ICH) as a field of research and site for digital efforts has grown significantly since the UNESCO 2003 Convention for the Safeguarding of Intangible Heritage. In contrast to tangible heritage, where cultural identities are manifested through physical objects, intangible cultural expressions are defined through tacit reliances and embodied practices. Such practices are usually bodily communicated, enacted, socially transmitted, and constantly evolving. Burgeoning trends in computational heritage and ICT applications have played a crucial role in safeguarding ICH as they produce versatile resources while making them accessible to the public. Nevertheless, most of the inventions are object-centric and cater to conserving material-based knowledge bases. Few endeavors thus far have fully supported the recording, representing, and reviving of the living nature of ICH.One of the challenges now faced is to find appropriate forms, together with efficient methods, to document the ephemeral aspects of intangible heritage. Another barrier is to find effective ways to communicate the knowledge inextricably linked to people. In response, recent efforts have embarked on capturing the “live” and “active” facets of the embodied cultures, which entails addressing technological and curatorial complexity to communicate the material and immaterial aspects within a meaningful context. Meanwhile, advancements in experimental museology have opened up new modes of experiential narratives, particularly through visualization, augmentation, participation, and immersive embodiment. Novel practices of cultural data computation and data sculpting have also emerged toward the ideal of knowledge reconstruction.This article outlines state-of-the-art models, projects, and technical practices that have advanced the digitization lifecycle for ICH resources. The review focuses on several critical but less studied tasks within digital archiving, computational encoding, conceptual representation, and interactive engagement with the intangible cultural elements. We aim at identifying the advancements and gaps in the existing conventions, and to envision opportunities for transmitting embodied knowledge in intangible heritage.",
     "accessed": {
      "day": 9,
      "month": 7,
      "year": 2024
     },
     "author": [
      {
       "family": "Hou",
       "given": "Yumeng"
      },
      {
       "family": "Kenderdine",
       "given": "Sarah"
      },
      {
       "family": "Picca",
       "given": "Davide"
      },
      {
       "family": "Egloff",
       "given": "Mattia"
      },
      {
       "family": "Adamou",
       "given": "Alessandro"
      }
     ],
     "container-title": "Journal on Computing and Cultural Heritage",
     "id": "6386835/M25WPB3U",
     "issue": "3",
     "issued": {
      "day": 16,
      "month": 9,
      "year": 2022
     },
     "page": "1-20",
     "page-first": "1",
     "shortTitle": "Digitizing Intangible Cultural Heritage Embodied",
     "title": "Digitizing Intangible Cultural Heritage Embodied: State of the Art",
     "title-short": "Digitizing Intangible Cultural Heritage Embodied",
     "type": "article-journal",
     "volume": "15"
    },
    "6386835/MJDE3VNJ": {
     "URL": "http://www.bruno-latour.fr/sites/default/files/53-BERLIN-PEDOFILpdf.pdf",
     "accessed": {
      "day": 23,
      "month": 10,
      "year": 2018
     },
     "author": [
      {
       "family": "Latour",
       "given": "Bruno"
      }
     ],
     "container-title": "Petites leçons de sociologie des sciences",
     "event-place": "Paris",
     "id": "6386835/MJDE3VNJ",
     "issued": {
      "year": 2006
     },
     "page": "171-224",
     "page-first": "171",
     "publisher": "La Découverte",
     "publisher-place": "Paris",
     "title": "Le \"pédofil\" de Boa Vista - montage photo-philosophique [1993]",
     "type": "chapter"
    },
    "6386835/MMEGRVTP": {
     "DOI": "10.1080/14794713.2021.1893491",
     "URL": "https://doi.org/10.1080/14794713.2021.1893491",
     "abstract": "The growing availability of video recordings of theatre performances, a phenomenon that has increased during Covid19 as theatres worldwide share videos online, affects the field of theatre and performance studies. Video recordings of theatre performances are archival documents and mediated ‘performance texts’ that enable new kinds of performance analysis and studying bodily practices. This paper addresses annotative methodologies as part of the research project, ‘The Art of Adaptation: The Theatre of Rina Yerushalmi and the Itim Ensemble.’ The project studies the video archive of the ensemble, which includes recordings of full productions and rehearsal processes. We discuss three kinds of digital comparative annotative methodologies to show how annotation can be used as a research tool for performance analysis: (1) Accumulative annotation: analysis of multi-layered theatrical sequences, revealing theatrical moments in their multiplicity. (2) Annotation of different scenes in a corpus of works: juxtaposing scenes from different productions enables the articulation of repetitive performative patterns, embodied practices, and visual images that construct a theatre language. (3) Annotation of the same scene at different phases: juxtaposing video recordings of the same scene at different moments of its development reveals nuanced sets of information about directorial choices, acting, movement, duration, and more.",
     "accessed": {
      "day": 27,
      "month": 9,
      "year": 2022
     },
     "author": [
      {
       "family": "Aronson-Lehavi",
       "given": "Sharon"
      },
      {
       "family": "Skop",
       "given": "Natan"
      },
      {
       "family": "Via Dorembus",
       "given": "Yael"
      }
     ],
     "container-title": "International Journal of Performance Arts and Digital Media",
     "id": "6386835/MMEGRVTP",
     "issue": "1",
     "issued": {
      "year": 2021
     },
     "note": "Publisher: Routledge\n_eprint: https://doi.org/10.1080/14794713.2021.1893491",
     "page": "86-101",
     "page-first": "86",
     "shortTitle": "Comparative video annotation and visual literacy",
     "title": "Comparative video annotation and visual literacy: performance analysis of Rina Yerushalmi’s theatre language",
     "title-short": "Comparative video annotation and visual literacy",
     "type": "article-journal",
     "volume": "17"
    },
    "6386835/MTWML2AM": {
     "URL": "https://digitalhumanities.org/dhq/vol/7/1/000143/000143.html",
     "abstract": "This article outlines a critical framework for a theory of performative materiality and its potential application to interface design from a humanistic perspective. Discussions of the materiality of digital media have become richer and more complex in the last decade, calling the literal, physical, and networked qualities of digital artifacts and systems to attention. This article extends those discussions by reconnecting them to a longer history of investigations of materiality and the specificity of media in critical theory and aesthetics. In addition, it introduces the concept of performative materiality, the enacted and event-based character of digital activity supported by those literal, physical conditions, and introduces the theoretical concerns that attach to that rubric. Performative materiality is based on the conviction that a system should be understood by what it does, not only how it is structured. As digital humanities matures, it can benefit from a re-engagement with the mainstream principles of critical theory on which a model of performative materiality is based. The article takes these ideas into a more focused look at how we might move towards integrating this model and critical principles into a model of humanistic interface design.",
     "accessed": {
      "day": 14,
      "month": 6,
      "year": 2024
     },
     "author": [
      {
       "family": "Drucker",
       "given": "Johanna"
      }
     ],
     "container-title": "Digital Humanities Quarterly",
     "id": "6386835/MTWML2AM",
     "issue": "1",
     "issued": {
      "year": 2013
     },
     "title": "Performative Materiality and Theoretical Approaches to Interface",
     "type": "article-journal",
     "volume": "7"
    },
    "6386835/N7D5UUTW": {
     "ISBN": "978-1-84788-637-8 978-1-84788-636-1",
     "abstract": "\"Design thinking is the core creative process for any designer; this book explores and explains this apparently mysterious 'design ability'. Focusing on what designers do when they design, Design Thinking is structured around a series of in-depth case studies of outstanding and expert designers at work, interwoven with overviews and analyses. The range covered reflects the breadth of design, from hardware and software design, to architecture and Formula One. The book offers new insights and understanding of design thinking, based on evidence from observation and investigation of design practice. Design Thinking is the distillation of the work of one of design's most influential thinkers. Nigel Cross goes to the heart of what it means to think and work as a designer. The book is an ideal guide for anyone who wants to be a designer or to know how good designers work in the field of contemporary design\"--",
     "author": [
      {
       "family": "Cross",
       "given": "Nigel"
      }
     ],
     "call-number": "NK1510 .C83 2011",
     "event-place": "Oxford ; New York",
     "id": "6386835/N7D5UUTW",
     "issued": {
      "year": 2011
     },
     "number-of-pages": "163",
     "publisher": "Bloomsbury/Berg",
     "publisher-place": "Oxford ; New York",
     "shortTitle": "Design thinking",
     "title": "Design thinking: understanding how designers think and work",
     "title-short": "Design thinking",
     "type": "book"
    },
    "6386835/NK8HP38Q": {
     "ISBN": "978-1-4724-4324-3",
     "abstract": "\"Data and its technologies now play a large and growing role in humanities research and teaching. This book addresses the needs of humanities scholars who seek deeper expertise in the area of data modeling and representation. The authors, all experts in digital humanities, offer a clear explanation of key technical principles, a grounded discussion of case studies, and an exploration of important theoretical concerns. The book opens with an orientation, giving the reader a history of data modeling in the humanities and a grounding in the technical concepts necessary to understand and engage with the second part of the book. The second part of the book is a wide-ranging exploration of topics central for a deeper understanding of data modeling in digital humanities. Chapters cover data modeling standards and the role they play in shaping digital humanities practice, traditional forms of modeling in the humanities and how they have been transformed by digital approaches, ontologies which seek to anchor meaning in digital humanities resources, and how data models inhabit the other analytical tools used in digital humanities research. It concludes with a glossary chapter that explains specific terms and concepts for data modeling in the digital humanities context. This book is a unique and invaluable resource for teaching and practising data modeling in a digital humanities context\"--",
     "call-number": "AZ105 .S43 2019",
     "collection-title": "Digital research in the arts and humanities",
     "editor": [
      {
       "family": "Flanders",
       "given": "Julia"
      },
      {
       "family": "Jannidis",
       "given": "Fotis"
      }
     ],
     "event-place": "London ; New York",
     "id": "6386835/NK8HP38Q",
     "issued": {
      "year": 2019
     },
     "language": "en",
     "number-of-pages": "341",
     "publisher": "Routledge, Taylor & Francis Group",
     "publisher-place": "London ; New York",
     "shortTitle": "The shape of data in the digital humanities",
     "title": "The shape of data in the digital humanities: modeling texts and text-based resources",
     "title-short": "The shape of data in the digital humanities",
     "type": "book"
    },
    "6386835/NSNHSUSL": {
     "URL": "https://litlab.stanford.edu/LiteraryLabPamphlet6.pdf",
     "accessed": {
      "day": 23,
      "month": 8,
      "year": 2019
     },
     "author": [
      {
       "family": "Moretti",
       "given": "Franco"
      }
     ],
     "id": "6386835/NSNHSUSL",
     "issued": {
      "year": 2013
     },
     "language": "en",
     "publisher": "Literary Lab, Stanford",
     "shortTitle": "“Operationalizing”",
     "title": "“Operationalizing”: or, the Function of Measurement in Modern Literary Theory. Pamphlet 6",
     "title-short": "“Operationalizing”",
     "type": "article"
    },
    "6386835/Q5RA5AZ4": {
     "ISBN": "978-1-84467-185-4",
     "author": [
      {
       "family": "Moretti",
       "given": "Franco"
      }
     ],
     "edition": "Paperback edition",
     "event-place": "London, New York",
     "id": "6386835/Q5RA5AZ4",
     "issued": {
      "year": 2005
     },
     "language": "eng",
     "note": "OCLC: 845372315",
     "number-of-pages": "119",
     "publisher": "Verso",
     "publisher-place": "London, New York",
     "shortTitle": "Graphs, maps, trees",
     "title": "Graphs, Maps, Trees: Abstract Models for a Literary History",
     "title-short": "Graphs, maps, trees",
     "type": "book"
    },
    "6386835/SLZ9AQ77": {
     "ISBN": "978-1-4833-6524-4",
     "author": [
      {
       "family": "Krueger",
       "given": "Richard A."
      },
      {
       "family": "Casey",
       "given": "Mary Anne"
      }
     ],
     "edition": "5th edition",
     "event-place": "Los Angeles London New Delhi Singapore Washington DC",
     "id": "6386835/SLZ9AQ77",
     "issued": {
      "year": 2015
     },
     "language": "eng",
     "number-of-pages": "252",
     "publisher": "SAGE",
     "publisher-place": "Los Angeles London New Delhi Singapore Washington DC",
     "shortTitle": "Focus groups",
     "title": "Focus groups: a practical guide for applied research",
     "title-short": "Focus groups",
     "type": "book"
    },
    "6386835/SQPWDX9P": {
     "URL": "https://hal.science/hal-04094170",
     "abstract": "De la fin du XIXe s. à la généralisation des images sur internet, la presse illustrée a été une force motrice déterminante pour la circulation des images. Elle a touché des publics élargis, diffusant par l’image des idées, des pratiques, des représentations. Mais s’il semble aisé d’étudier la rencontre et le métissage de quelques images, comment comprendre les circulations visuelles à grande échelle? Comment étudier la mondialisation par l’image sans se contenter d’études de cas ? Tel est le défi du projet Visual Contagions, mené par la chaire d’humanités numériques de l’université de Genève avec le soutien du Fonds national suisse.",
     "accessed": {
      "day": 17,
      "month": 7,
      "year": 2024
     },
     "author": [
      {
       "family": "Carboni",
       "given": "Nicola"
      },
      {
       "family": "Barras",
       "given": "Marie"
      },
      {
       "family": "Joyeux-Prunel",
       "given": "Béatrice"
      }
     ],
     "collection-title": "Images",
     "container-title": "Humanistica 2023",
     "event-place": "Genève, Switzerland",
     "id": "6386835/SQPWDX9P",
     "issued": {
      "month": 6,
      "year": 2023
     },
     "publisher": "Association francophone des humanités numériques",
     "publisher-place": "Genève, Switzerland",
     "title": "Pister des circulations visuelles à l'échelle mondiale",
     "type": "paper-conference"
    },
    "6386835/T4VBK6L6": {
     "URL": "https://hal.science/hal-03828472",
     "abstract": "Funded through the ERASMUS+ program, COSMIC (CO-creating and Sharing digital Methodologies In Circus Education) is a project led by the French Federation of Circus Schools (FFEC) and the European Federation of Professional Circus Schools (FEDEC). In all, eleven partners were involved in COSMIC, including nine circus schools from seven different countries as well as associated partners. The project took place from September 2020 to December 2022. Its aim is to introduce European professional circus schools to the use of digital technology in order to offer innovative teaching methods. The starting point for COSMIC was to initiate a discussion to develop a new design of the “Student Artistic Booklet” using an open-source video annotation web application: MemoRekall. In return, the dialogue with the application design team needed to make it possible to develop new features that meet the needs of circus education. Indeed, although MemoRekall was designed for the performing arts, a co-design approach implemented throughout the COSMIC project has enabled it to evolve to take into account the specific characteristics of the circus, which is as much a performing art as it is a top-level sport. The purpose of this white paper is to review the different stages and challenges of the collaboration between the circus school teaching teams and students with MemoRekall’s research and development team. In particular, it presents the contents of the labs, the methodologies used, the technological choices made, and the implementation of different teaching scenarios. Publication available in English and French.",
     "accessed": {
      "day": 18,
      "month": 4,
      "year": 2023
     },
     "author": [
      {
       "family": "Bardiot",
       "given": "Clarisse"
      },
      {
       "family": "Menassel",
       "given": "Mei"
      }
     ],
     "id": "6386835/T4VBK6L6",
     "issued": {
      "year": 2022
     },
     "language": "en",
     "publisher": "Rennes 2",
     "title": "Video Annotation for Circus Education. MemoRekall and the COSMIC projet",
     "type": "article"
    },
    "6386835/TYGJBIQA": {
     "ISBN": "978-0-230-00256-2 978-0-230-30234-1",
     "author": [
      {
       "family": "Bateman",
       "given": "John A."
      }
     ],
     "edition": "1. paperback ed",
     "event-place": "Basingstoke, Hampshire",
     "id": "6386835/TYGJBIQA",
     "issued": {
      "year": 2008
     },
     "language": "eng",
     "number-of-pages": "312",
     "publisher": "Palgrave Macmillan",
     "publisher-place": "Basingstoke, Hampshire",
     "shortTitle": "Multimodality and genre",
     "title": "Multimodality and genre: a foundation for the systematic analysis of multimodal documents",
     "title-short": "Multimodality and genre",
     "type": "book"
    },
    "6386835/UZKX34LS": {
     "ISBN": "978-0-262-54613-3",
     "abstract": "\"The authors construct theories and methods for \"distant viewing\" as a method for analyzing collections of digitized visual materials with computer vision\"--",
     "author": [
      {
       "family": "Arnold",
       "given": "Taylor"
      },
      {
       "family": "Tilton",
       "given": "Lauren"
      }
     ],
     "call-number": "TA1634 .A76 2023",
     "event-place": "Cambridge, Massachusetts",
     "id": "6386835/UZKX34LS",
     "issued": {
      "year": 2023
     },
     "publisher": "The MIT Press",
     "publisher-place": "Cambridge, Massachusetts",
     "shortTitle": "Distant viewing",
     "title": "Distant viewing: computational exploration of digital images",
     "title-short": "Distant viewing",
     "type": "book"
    },
    "6386835/VBD8BYQJ": {
     "DOI": "10.5281/zenodo.6619635",
     "URL": "https://zenodo.org/record/6619635",
     "abstract": "This document provides a brief overview of MemoRekall (www.memorekall.com) -  a free and open-source web app to explain and annotate a video enhanced by adding notes, documents, or web links - and  how it was used during the COESO project Pilot 2, “Dancing Philosophy”, to document the creative process, the collaboration between the choreographer and the philosopher, and the workshops with the students.",
     "accessed": {
      "day": 1,
      "month": 11,
      "year": 2022
     },
     "author": [
      {
       "family": "Bardiot",
       "given": "Clarisse"
      },
      {
       "family": "Hildebrand",
       "given": "Sébastien"
      },
      {
       "family": "Ferrando",
       "given": "Stefania"
      },
      {
       "family": "Graffione",
       "given": "Cosetta"
      },
      {
       "family": "Blin",
       "given": "Irénée"
      },
      {
       "family": "Marranca",
       "given": "Daniele"
      }
     ],
     "id": "6386835/VBD8BYQJ",
     "issued": {
      "day": 30,
      "month": 5,
      "year": 2022
     },
     "language": "eng",
     "note": "Publisher: Zenodo",
     "shortTitle": "Documenting collaborations with video annotation",
     "title": "Documenting collaborations with video annotation: MemoRekall Capsules (COESO D.2.4)",
     "title-short": "Documenting collaborations with video annotation",
     "type": "article-journal"
    },
    "6386835/VNXFSQUM": {
     "DOI": "10.5281/zenodo.6788039",
     "URL": "https://zenodo.org/record/6788039",
     "abstract": "This report is a preprint paper presenting and studying the cooperative research carried out by the Dancing Philosophy pilot project, developed within the framework of the COESO project (Collaborative Engagement on Societal Issues) coordinated by Open Edition.",
     "accessed": {
      "day": 3,
      "month": 4,
      "year": 2023
     },
     "author": [
      {
       "family": "Ferrando",
       "given": "Stefania"
      }
     ],
     "id": "6386835/VNXFSQUM",
     "issued": {
      "day": 29,
      "month": 6,
      "year": 2022
     },
     "language": "eng",
     "note": "Publisher: Zenodo",
     "shortTitle": "The movement of an embodied thought",
     "title": "The movement of an embodied thought: Pilot 2 Dancing Philosophy Report (COESO D.2.6)",
     "title-short": "The movement of an embodied thought",
     "type": "article-journal"
    },
    "6386835/X2IS589E": {
     "ISBN": "978-1-78630-705-7",
     "author": [
      {
       "family": "Bardiot",
       "given": "Clarisse"
      }
     ],
     "event-place": "Hoboken",
     "id": "6386835/X2IS589E",
     "issued": {
      "year": 2021
     },
     "publisher": "ISTE / Wiley",
     "publisher-place": "Hoboken",
     "shortTitle": "Performing arts and digital humanities",
     "title": "Performing arts and digital humanities: from traces to data",
     "title-short": "Performing arts and digital humanities",
     "type": "book"
    },
    "6386835/YDW75SLU": {
     "DOI": "10.1080/01615440.2024.2344004",
     "URL": "https://doi.org/10.1080/01615440.2024.2344004",
     "abstract": "Semantically enriched historical newspapers offer a multitude of opportunities for data-driven exploration and analysis. In this paper we introduce the impresso interface which integrates several types of semantic enrichments and data visualization and thereby supports new exploratory workflows and the critical assessment of large-scale digitized source collections. The interface targets historians and integrates search, filtering, comparison, and recommendation based on automatically detected topics, linked named entities, text reuse, n-grams, image similarity, language, and OCR quality. We introduce the theoretical principles which guided interface development and reflect on the user requirements gathering process together with a case-study driven exemplification of novel workflows facilitated by the interface. We conclude with an overview of accompanying educational materials and discuss results from a user evaluation.",
     "accessed": {
      "day": 9,
      "month": 7,
      "year": 2024
     },
     "author": [
      {
       "family": "Düring",
       "given": "Marten"
      },
      {
       "family": "Bunout",
       "given": "Estelle"
      },
      {
       "family": "Guido",
       "given": "Daniele"
      }
     ],
     "container-title": "Historical Methods: A Journal of Quantitative and Interdisciplinary History",
     "id": "6386835/YDW75SLU",
     "issue": "0",
     "issued": {
      "year": 2024
     },
     "note": "Publisher: Routledge\n_eprint: https://doi.org/10.1080/01615440.2024.2344004",
     "page": "35–55",
     "page-first": "35",
     "title": "Transparent generosity. Introducing the impresso interface for the exploration of semantically enriched historical newspapers",
     "type": "article-journal",
     "volume": "0"
    },
    "6386835/ZM8SK4WQ": {
     "DOI": "10.1093/llc/fqad008",
     "URL": "https://doi.org/10.1093/llc/fqad008",
     "abstract": "Until recently, most research in the Digital Humanities (DH) was monomodal, meaning that the object of analysis was either textual or visual. Seeking to integrate multimodality theory into the DH, this article demonstrates that recently developed multimodal deep learning models, such as Contrastive Language Image Pre-training (CLIP), offer new possibilities to explore and analyze image–text combinations at scale. These models, which are trained on image and text pairs, can be applied to a wide range of text-to-image, image-to-image, and image-to-text prediction tasks. Moreover, multimodal models show high accuracy in zero-shot classification, i.e. predicting unseen categories across heterogeneous datasets. Based on three exploratory case studies, we argue that this zero-shot capability opens up the way for a multimodal turn in DH research. Moreover, multimodal models allow scholars to move past the artificial separation of text and images that was dominant in the field and analyze multimodal meaning at scale. However, we also need to be aware of the specific (historical) bias of multimodal deep learning that stems from biases in the training data used to train these models.",
     "accessed": {
      "day": 31,
      "month": 3,
      "year": 2023
     },
     "author": [
      {
       "family": "Smits",
       "given": "Thomas"
      },
      {
       "family": "Wevers",
       "given": "Melvin"
      }
     ],
     "container-title": "Digital Scholarship in the Humanities",
     "container-title-short": "DSH",
     "id": "6386835/ZM8SK4WQ",
     "issued": {
      "year": 2023
     },
     "journalAbbreviation": "DSH",
     "page": "1267–1280",
     "page-first": "1267",
     "title": "A multimodal turn in Digital Humanities. Using contrastive machine learning models to explore, enrich, and analyze digital visual historical collections",
     "type": "article-journal",
     "volume": "38"
    },
    "undefined": {
     "URL": "http://www.digitalhumanities.org/dhq/vol/14/3/000476/000476.html",
     "author": [
      {
       "family": "Bardiot",
       "given": "Clarisse"
      }
     ],
     "container-title": "Digital Humanities Quarterly",
     "container-title-short": "DHQ",
     "id": "6386835/7R9S2WPF",
     "issue": "3",
     "issued": {
      "year": 2020
     },
     "journalAbbreviation": "DHQ",
     "shortTitle": "Theatre analytics",
     "title": "Theatre analytics: developing software for theatre research",
     "title-short": "Theatre analytics",
     "type": "article-journal",
     "volume": "14"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
